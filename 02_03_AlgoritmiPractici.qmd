
## II.5 Detecția semnalelor cu distribuții necunoscute

### Distribuții necunoscute

- Până acum, se cunoștea dpdv. matematic statistica
tuturor datelor:

  - Se cunoșteau semnalele:

    - $s_0(t) = ...$
    - $s_1(t) = ...$

  - Se cunoștea zgomotul

    - gaussian, uniform, etc.

  - Se cunoșteau distribuțiile condiționate:

    - $w(r|H_0) = ...$
    - $w(r|H_1) = ...$

- În aplicații reale, lucrurile pot fi mai complicate

### Exemplu

- Dacă semnalele $s_0(t)$ și $s_1(t)$
nu există / nu se cunosc?

- Exemplu: recunoașterea feței unei persoane

    - Identificarea persoanei A sau B bazată pe o imagine a feței
    - Avem:
        - 100 imagini ale persoanei A, în condiții diverse
        - 100 imagini ale persoanei B, în condiții diverse

### Eșantioane vs distribuții

- Să comparăm recunoașterea fețelor cu detecția semnalelor

- Aspecte comune:

    - două ipoteze $H_0$ (persoana A) și $H_1$ (persoana B)
    - un vector de eșantioane $\vec{r}$ = imaginea pe baza căreia se face decizia
    - se pot lua două decizii
    - 4 scenarii: rejecție corectă, alarmă falsă, pierdere, detecție corectă

- Ce diferă? Nu există formule matematice

    - nu există semnalele "originale" $s_0(t) = ...$ și $s_1(t)...$
    - (fețele persoanelor A și B nu pot fi exprimate matematic ca semnale)
    - în schimb, avem multe exemple din fiecare distribuție

        - 100 imagini ale lui A = exemple ale $\vec{r}$ în ipoteza $H_0$
        - 100 imagini ale lui B = exemple ale $\vec{r}$ în ipoteza $H_1$

### Terminologie

- Terminologia folosită în domeniul **învățării automate** (*machine learning*):

    - Acest tip de problemă = problemă de **clasificare** a semnalelor
        - se dă un vector de date, găsiți-i clasa

    - **Clase de semnal** = categoriile posibile ale semnalelor (ipotezele $H_i$, persoanele A/B etc)

    - **Set de antrenare** = un set de semnale cunoscute inițial

        - de ex. 100 de imagini ale fiecărei persoane
        - setul de date va fi folosit în procesul de decizie


### Eșantioane și distribuții

- Setul de antrenare conține informațiile
pe care le-ar conține distribuțiile condiționate $w(r|H_0)$ și $w(r|H_1)$

    - $w(r|H_0)$ exprimă cum arată valorile lui $r$ în ipoteza $H_0$
    - $w(r|H_1)$ exprimă cum arată valorile lui $r$ în ipoteza $H_1$
    - setul de antrenare exprimă același lucru, nu prin formule, dar prin multe exemple

- Cum se face clasificarea în aceste condiții?

### Algoritmul k-NN

Algoritmul *k-Nearest Neighbours* (k-NN)

- Intrare:

    - set de antrenare cu vectorii $\vec{x}_1 ... \vec{x}_N$,
    din $L$ clase posibile de semnal $C_1$...$C_L$
    - clasele vectorilor de antrenare sunt cunoscute
    - vector de test $\vec{r}$ care trebuie clasificat
    - parametrul $k$

1. Se calculează distanța între $\vec{r}$ și fiecare vector de antrenare $\vec{x}_i$
    - se poate utiliza distanța Euclidiană, aceeași utilizată
    pentru detecția semnalelor din secțiunile precedente

2. Se aleg cei mai apropiați $k$ vectori de $\vec{r}$ (cei *$k$ "nearest neighbours*")

3. Se determină clasa lui $\vec{r}$ = clasa majoritară între cei $k$ cei mai apropiați vecini

- Ieșire: clasa vectorului $\vec{r}$


### Algoritmul k-NN

![Algoritmul k-NN ilustrat [1]](img/kNN.png){#id .class width=58%}

\maketiny{
[1] sursa: "KNN Classification using Scikit-learn", Avinash Navlani, https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn
}


### Algoritmul k-NN și decizia ML

- Dacă setul de antrenare este foarte mare, algoritmul k-NN devine simular cu decizia pe baza criteriului ML

- Numărul de vectori situați într-o vecinătate a unui punct $r$ este proporțional cu $w(r|H_i)$

- Mai mulți vecini din clasa A decât din clasa B $\Leftrightarrow$ $w(r|H_A)$ > $w(r|H_B)$

### Algoritmul k-NN și decizia ML

- Exemplu: frunze și copaci :) de povestit

### Exercițiu

Exercițiu

1. Fie următorul set de antrenare, compus din
5 vectori din clasa A și alți 5 vectori din clasa B:
    - Clasa A:
    $$
\vec{v}_1 = \begin{bmatrix}  1 \\ -2 \end{bmatrix}\;
\vec{v}_2 = \begin{bmatrix} -1 \\  1 \end{bmatrix}\;
\vec{v}_3 = \begin{bmatrix} -4 \\  2 \end{bmatrix}\;
\vec{v}_4 = \begin{bmatrix}  2 \\  1 \end{bmatrix}\;
\vec{v}_5 = \begin{bmatrix} -2 \\ -2 \end{bmatrix}$$

    - Clasa B:
    $$
\vec{v}_6    = \begin{bmatrix}  7 \\ 0 \end{bmatrix}\;
\vec{v}_7    = \begin{bmatrix}  2 \\ 3 \end{bmatrix}\;
\vec{v}_8    = \begin{bmatrix}  3 \\ 2 \end{bmatrix}\;
\vec{v}_9    = \begin{bmatrix} -3 \\ 8 \end{bmatrix}\;
\vec{v}_{10} = \begin{bmatrix} -2 \\ 5 \end{bmatrix}$$

    Determinați clasa vectorului $\vec{x} = \begin{bmatrix} -3 \\ 6 \end{bmatrix}$
utilizând algoritmul k-NN, cu $k=1$, $k=3$, $k=5$, $k=7$ and $k=9$

### Discuție

- k-NN este un algoritm de învățare supervizată
    - se cunosc clasele vectorilor din setul de antrenare

\smallskip

- Efectul lui $k$: netezirea frontierei de decizie:
    - $k$ mic: frontieră foarte cotită / "șifonată" / cu multe coturi
    - $k$ mare: frontieră mai netedă

\smallskip

- Cum se găsește o valoare optimă pentru  $k$?

### *Cross-validation*

- Cum se găsește o valoare optimă pentru  $k$?
    - prin încercări ("băbește")

\smallskip

- "**Cross-validation**" = folosirea unui mic set de test pentru a verifica care valoare a parametrului e mai bună

    - acest set de date se numește set de "**cross-validare**"
    - se impune $k=1$, se testează cu setul de "*cross-validare*" câți vectori sunt clasificați corect
    - se repetă pentru $k=2, 3, ... max$
    - se alege valoarea lui $k$ cu care s-au obținut rezultatele cele mai bune

### Evaluarea algoritmilor

- Cum se evaluează performanța algoritmului k-NN?
    - Se folosește un set de date de testare, și se calculează procentajul vectorilor clasificați corect

\smallskip

- Setul de date pentru evaluarea finală trebuie să fie diferit de setul de "*cross-validare*"
    - pentru evaluarea finală se folosesc date pe care algoritmul nu le-a mai utilizat niciodată

\smallskip

- Cum se împarte setul de date disponibile?


### Seturi de date

- Presupunem că avem în total 200 imagini tip fețe, 100 imagini ale persoanei A și 100 ale lui B

\smallskip

- Setul de date total se împarte în:
	\smallskip
    - Set de antrenare
        - vectorii care vor fi utilizați de algoritm
        - cel mai numeros, aprox. 60% din datele totale
        - de ex. 60 imagini ale persoanei A și 60 ale lui B
    \smallskip
    - Set de *cross-validare*
        - utilizat pentru a testa algoritmul în vederea alegerii parametrilor optimi ($k$)
        - mai mic, aprox. 20% din date (de ex. 20 imagini ale lui of A și 20 ale lui B)
    \smallskip
    - Set de testare
        - utilizat pentru evaluarea finală a algoritmului, cu valorile parametrilor fixate
        - mai mic, aprox. 20% din date (de ex. 20 imagini ale lui of A și 20 ale lui B)

### Algoritmul *k-Means*

- k-Means: un algoritm pentru ***clusterizarea*** datelor
    - identificarea grupurilor de date apropiate între ele

\smallskip

- Un exemplu de algoritm de învățare nesupervizată
    - "învățare nesupervizată" = nu se cunosc clasele semnalelor din setul de antrenare

### Algoritmul *k-Means*

Algoritmul *k-Means*

- Intrare:
    - set de antrenare cu vectorii $\vec{x}_1 ... \vec{x}_N$
    - numărul de clase C

- Inițializare: centroizii C iau valori aleatoare
	$$\vec{c}_i \leftarrow \textrm{ valori aleatoare }$$
- Repetă
  1. Clasificare: se clasifică fiecare vector $\vec{x}$ pe baza celui mai apropiat centroid:
	    $$class{x} = \arg\min_i d(\vec{x}, \vec{c}_i), \forall \vec{x}$$
  2. Actualizare: se actualizează centroizii $\vec{c}_i$ = media vectorilor $\vec{x}$ din clasa $i$
	    $$\vec{c}_i \leftarrow \textrm{ media vectorilor } \vec{x}, \forall \vec{x} \textrm{ din clasa } i$$

- Ieșire: centroizii $\vec{c}_i$, clasele tuturor vectorilor  de intrare $\vec{x}_n$


### Algoritmul *k-Means*

Algoritmul *k-Means* explicat video:

- Urmăriți video-ul următor, de la 6:28 to 7:08

    [https: //www.youtube.com/watch?v=4b5d3muPQmA](https://www.youtube.com/watch?v=4b5d3muPQmA)

\smallskip

- Urmăriți video-ul următor, de la 3:05 la final

    [https: //www.youtube.com/watch?v=IuRb3y8qKX4](https://www.youtube.com/watch?v=IuRb3y8qKX4)


### Algoritmul *k-Means*

- Algoritmul *k-Means* poate să nu conveargă spre niște grupuri adecvate de date
    - rezultatele depind de inițializarea aleatoare a centroizilor
    - se rulează de mai multe ori, se alege cel mai bun rezultat
    - există metode de inițializare optimizate (*k-Means++*)


### Exercițiu

Exercițiu

1. Fie datele următoare:
    $$\left\lbrace \vec{v_n} \right\rbrace =
[ 1.3, -0.1, 0.5, 4.7, 5.1, 5.8, 0.4, 4.8, -0.7, 4.9 ] $$

    Utilizați algoritmul k-Means pentru a găsi doi centroizi $\vec{c}_1$ și $\vec{c}_2$,
pornind de la valorile aleatoare $\vec{c}_1 = -0.5$ și $\vec{c}_2 = 0.9$.
Realizați 5 iterații ale algoritmului.

