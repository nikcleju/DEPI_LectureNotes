[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DEPI",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "01_Intro.html",
    "href": "01_Intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "See Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "01_SemnaleAleatoare.html",
    "href": "01_SemnaleAleatoare.html",
    "title": "2  Semnale aleatoare",
    "section": "",
    "text": "O variabilă aleatoare (v.a.) este o variabilă care denumește o valoare produsă printr-un fenomen aleator. Practic, reprezintă un nume \\(X\\), \\(Y\\) etc. atașat unei valori arbitrare.\nO realizare a unei variabile aleatoare este o valoare particulară posibilă pe care o poate lua respectiva variabilă.\nSpațiul realizărilor \\(\\Omega\\) este mulțimea tuturor realizărilor (mulțimea tuturor valorilor posibile ale unei variabile aleatoare).\nExemple:\n\nFie \\(X\\) = numărul obținut prin aruncarea unui zar. Spațiul realizărilor este \\[\\Omega = \\left\\{1, 2, 3, 4, 5, 6\\right\\}\\]\nFie \\(V_{in}\\) = voltajul măsurat al unei baterii. Spațiul realizărilor este \\[\\Omega = [1, 1.7] V\\]\nFie \\(X\\) = rezultatul obținut la aruncarea unei monede, notat cu 0 sau 1. \n(sursa imaginii: https://www.mathsisfun.com/data/random-variables.html)\n\nVariabilele aleatoare modelează semnale afectate de zgomot, cum ar fi un semnal de tensiune într-un circuit electric (zgomot termic etc), sau imagini afectate de zgomot\n\n\n\nSemnal sinusoidal\n\nimport matplotlib.pyplot as plt, numpy as np, math;\nx = np.linspace(0, 99, 100);\ns = np.sin(2*math.pi*0.02*x)\nplt.figure(figsize=(10,6));\nplt.plot(x,s);\nplt.ylim(-3,3)\nplt.xlabel('t');\nplt.ylabel('sin(x)');\nplt.title('Sinusiodal signal');\nplt.savefig('fig/01_RandomSignals_Sine.png', transparent=True, bbox_inches='tight', dpi=300)\nplt.close()\n\n\nSemnal sinusoidal + zgomot (distribuție normală, \\(\\mu = 0, \\sigma^2 = 1\\))\n\nimport matplotlib.pyplot as plt, numpy as np, math;\nx = np.linspace(0, 99, 100);\ns = np.sin(2*math.pi*0.02*x)\nsn = s + np.random.randn(100)\nplt.plot(x,s, x, sn);\nplt.ylim(-3,3)\nplt.xlabel('t');\nplt.ylabel('sin(x)');\nplt.title('Sinusiodal signal + random noise');\nplt.savefig('fig/01_RandomSignals_SinePlusRandn.png', transparent=True, bbox_inches='tight', dpi=300)\nplt.close()\n\n\nSemnal sinusoidal + zgomot + zgomot (distribuție uniformă \\(\\mathcal{U} [-1,1]\\))\nCe diferă? Tipul distribuției\n\nimport matplotlib.pyplot as plt, numpy as np, math;\nx = np.linspace(0, 99, 100);\ns = np.sin(2*math.pi*0.02*x)\nsn = s + np.random.uniform(-1,1,100)\nplt.plot(x,s,x,sn);\nplt.ylim(-3,3)\nplt.xlabel('t');\nplt.ylabel('sin(x)');\nplt.title('Sinusiodal signal  + random noise');\nplt.savefig('fig/01_RandomSignals_SinePlusRand.png', transparent=True, bbox_inches='tight', dpi=300)\nplt.close()\n\n\nImagine originală\n\nimport matplotlib.pyplot as plt, numpy as np, math, PIL;\nfrom PIL import Image\nmyImage = Image.open(\"img/TestImageGirl.gif\").convert(\"L\");\nim = np.array(myImage)\nplt.imshow(im, cmap='gray', vmin=0, vmax=255)\nplt.savefig('fig/01_RandomSignals_ImageClean.png', transparent=True, bbox_inches='tight', dpi=300)\nplt.close()\n\n\nImagine + zgomot (normal, \\(\\mu = 0, \\sigma^2 = 1\\))\n\nimport matplotlib.pyplot as plt, numpy as np, math, PIL;\nfrom PIL import Image\nmyImage = Image.open(\"img/TestImageGirl.gif\").convert(\"L\");\nim = np.array(myImage)\nsigma = math.sqrt(225);\nimn = im + sigma*np.random.randn(im.shape[0], im.shape[1])\nplt.imshow(imn, cmap='gray', vmin=0, vmax=255)\nplt.savefig('fig/01_RandomSignals_ImageRandn1.png', transparent=True, bbox_inches='tight', dpi=300)\nplt.close()\n\n\nImagine + zgomot mai mare (normal, \\(\\mu = 0, \\sigma^2 = 10\\))\n\nimport matplotlib.pyplot as plt, numpy as np, math, PIL;\nfrom PIL import Image\nmyImage = Image.open(\"img/TestImageGirl.gif\").convert(\"L\");\nim = np.array(myImage)\nsigma = math.sqrt(1500);\nimn = im + sigma*np.random.randn(im.shape[0], im.shape[1])\nplt.imshow(imn, cmap='gray', vmin=0, vmax=255)\nplt.savefig('fig/01_RandomSignals_ImageRandn2.png', transparent=True, bbox_inches='tight', dpi=300)\nplt.close()\n\n\nImagine + zgomot (uniform, \\(\\mathcal{U} [-5, 5]\\))\n\nimport matplotlib.pyplot as plt, numpy as np, math, PIL;\nfrom PIL import Image\nmyImage = Image.open(\"img/TestImageGirl.gif\").convert(\"L\");\nim = np.array(myImage)\nimn = im + sigma*np.random.uniform(-5, 5, im.shape)\nplt.imshow(imn, cmap='gray', vmin=0, vmax=255)\nplt.savefig('fig/01_RandomSignals_ImageRandUnif.png', transparent=True, bbox_inches='tight', dpi=300)\nplt.close()\n\n\n\n\nO variabilă aleatoare este discretă dacă \\(\\Omega\\) este o mulțime discretă, sau continuă dacă \\(\\Omega\\) este o mulțime compactă.\nNumărul \\(X\\) obținut prin aruncarea unui zar este o v.a. discretă. Valoarea măsurată a tensiunii este o v.a. continuă."
  },
  {
    "objectID": "01_SemnaleAleatoare.html#probabilitate-și-densitate-de-probabilitate",
    "href": "01_SemnaleAleatoare.html#probabilitate-și-densitate-de-probabilitate",
    "title": "2  Semnale aleatoare",
    "section": "2.2 Probabilitate și densitate de probabilitate",
    "text": "2.2 Probabilitate și densitate de probabilitate\n\n2.2.1 Pentru variabile aleatoare discrete\nVariabilele aleatoare discrete sunt caracterizate prin două funcții: funcția masă de probabilitate (discretă) și funcția de repartiție (care e “în trepte”).\nFie o v.a. discretă \\(A\\).\nSe definește funcția masă de probabilitate (FMP) (“probability mass function”, lb.eng.) \\(w_A(x)\\) ca fiind probabilitatea ca \\(A\\) să aibă valoarea egală cu \\(x\\): \\[w_A(x)= P \\left\\{A = x \\right\\}\\]\nExemplu: funcția masă de probabilitate pentru valoarile unui zar este următoarea:\n\n\nFMP mai e numită, în limbaj uzual, “distribuția” variabilei aleatoare.\nFMP se poate utiliza pentru a calcula probabilitatea unor valori:\n\nProbabilitatea ca \\(A\\) să aibă valoarea \\(v\\): \\[P\\left\\{ A = v\\right\\} = w_A(v)\\]\nProbabilitatea ca A să fie între valorile \\(a\\) și \\(b\\) (inclusiv): \\[P\\left\\{ a \\leq A \\leq b\\right\\} = \\sum_{x=a}^b w_A(x)\\]\n\nFuncția de repartiție (FR) \\(F_A(x)\\) a unei variabile aleatoare \\(A\\) reprezintă probabilitatea ca \\(A\\) să aibă valoarea mai mică sau egală cu \\(x\\): \\[F_A(x) = P\\left\\{ A \\leq x \\right\\}\\]\nExemplu: funcția de repartiție a valorilor unui zar este următoarea:\n\n\nPentru v.a. discrete, FR este întotdeauna “în trepte”.\nȘi funcția de repartiție poate fi folosită pentru a calcula diverse probabiități:\n\nProbabilitatea ca \\(A\\) să aibă valoarea \\(v\\): \\[P\\left\\{ A = v\\right\\} = F_A(v) - F_A(v-1)\\]\nProbabilitatea ca A să fie între valorile \\(a\\) și \\(b\\) (inclusiv): \\[P\\left\\{ a \\leq A \\leq b\\right\\} = P\\left\\{ A \\leq b\\right\\} - P\\left\\{ A \\leq a\\right\\} = F_A(b) - F_A(a-1)\\]\n\nÎntre funcția masă de probabilitate și funcția de repartiție există următoarea legătură. FR este suma cumulativă (un fel de “integrală discretă”) a FMP: \\[F_A(x) = \\sum_{t = -\\infty}^{t = x} w_A(t)\\]\nExemplu pentru zar: grafic, la tablă\n\n\n2.2.2 Pentru variabile aleatoare continue\nVariabilele aleatoare continue sunt caracterizate prin două funcții: funcția densitate de probabilitate și funcția de repartiție.\nFie o variabilă aleatoare continuă \\(A\\).\nFuncția densitate de probabilitate (FDP) \\(w_A(x)\\) se definește ca probabilitatea ca valoarea lui \\(A\\) să fie într-o vecinătate \\(\\epsilon\\) mică în jurul lui \\(x\\), împărțit la \\(\\epsilon\\) \\[w_A(x) = \\lim_{\\epsilon \\to 0}{\\frac{P(A \\in [x, x+\\epsilon])}{\\epsilon}}\\]\n\n\nÎn limbaj informal, se mai numește și distribuția variabilei A.\nFuncția de repartiție (FR) \\(F_A(x)\\) se definește la fel ca la variabile discrete, adică reprezintă probabilitatea ca \\(A\\) să aibă valoarea mai mică sau egală cu \\(x\\): \\[F_A(x) = P\\left\\{ A \\leq x \\right\\}\\]\nÎntre cele două funcții există următoarea legătură. Funcția de repartiție este integrala densității de probabilitate, sau, altfel spus, densitatea de probabilitate este derivata funcției de repartiție: \\[F_A(x) = \\int_{-\\infty}^x w_A(u) \\mathrm{d}u\\]\n\\[\\begin{split}\nw_A(x) &= \\lim_{\\epsilon \\to 0}{\\frac{P(A \\in [x, x+\\epsilon])}{\\epsilon}} \\\\\n&= \\lim_{\\epsilon \\to 0}{\\frac{P(A \\leq x+\\epsilon) - P(A \\leq x)}{\\epsilon}} \\\\\n&= \\lim_{\\epsilon \\to 0}{\\frac{F_A(x+\\epsilon) - F_A(x)}{\\epsilon}} \\\\\n&= \\frac{\\mathrm{d}F_A(x)}{\\mathrm{d}x} = F'_A(x)\n\\end{split}\\]\n\n\nCa în fizică: \\[\\rho(x) = \\frac{dM}{dV}\\] densitatea unui punct = masa unei mic volum V în jurul acelui punct împărțit la volumul V, pentru \\(V \\to 0\\).\nDensitatea de probabilitate se poate folosi pentru a calcula probabilități, dar numai prin integrare:\n\nProbabilitatea ca A să fie între valorile \\(a\\) și \\(b\\) = integrala FDP între \\(a\\) și \\(b\\): \\[P\\left\\{ a \\leq A \\leq b\\right\\} = \\int_a^b w_A(x) dx\\]\n\n(sursa: “https://intellipaat.com/blog/tutorial/statistics-and-probability-tutorial/probability-distributions-of-continuous-variables/*)\nProbabilitatea ca \\(A\\) să aibă exact valoarea \\(v\\) este întotdeauna 0 \\[P\\left\\{ A = v\\right\\} = \\int_v^v w_A(x) dx = 0\\]\n(în interpretarea grafică, aria de sub un punct este nulă)\n\nFuncția de repartiție se poate folosi de asemenea pentru calculul unor probabilități, în mod direct, la fel ca la variabile discrete:\n\nProbabilitatea ca valoarea lui A să fie între \\(a\\) și \\(b\\): \\[P\\left\\{ a \\leq A \\leq b\\right\\} = F_A(b) - F_A(a)\\]\nNu contează dacă se consideră un interval deschis sau închis: \\[P\\left\\{ a \\leq A \\leq b\\right\\} = P\\left\\{ a < A < b\\right\\}\\]\n\n\n\n2.2.3 Interpretarea densității de probabilitate\nO variabilă aleatoare continuă poate lua o infinitate de valori posibile.\nAșadar, probabilitatea ca o variabilă aleatoare continuă \\(A\\) să ia exact o anume valoare precisă \\(x\\), dintr-o infinitate de alternative, este întotdeauna zero.\nAtunci ce ne spune densitatea de probabilitate \\(w_A(x)\\)?\nDensitatea de probabilitate \\(w_A(x)\\) într-un punct \\(x = x_1\\) ne spune probabilitatea ca \\(A\\) să ia valori în jurul acelei valori \\(x_1\\), comparativ cu a lua valori în jurul unei alte valori \\(x_2\\), și nu reprezintă probabilitatea unei valori anume, care este mereu zero.\nÎn consecință valorile lui \\(w_A(x)\\) se pot folosi în mod direct pentru comparații (“E mai probabil ca \\(A\\) să ia valori în jurul lui 5, sau în jurul lui 7?”), dar pentru calculul unor probabiltăți trebuie să apelăm mereu la integrala sa.\nLa variabile aleatoare discrete, funcția masă de probabilitate joacă același rol ca densitatea de probabilitate, cu diferența că ea ne dă chiar probabilitatea unei valori \\(x\\), nu doar probabilitatea de a fi în jurul lui \\(x\\). De asemenea, fiind funcții discrete, operația de integrare se înlocuiește cu o sumă.\n\n(sursa imaginii: “Probability Distributions: Discrete and Continuous”, Seema Singh, https://towardsdatascience.com/probability-distributions-discrete-and-continuous-7a94ede66dc0)\n\n\n2.2.4 Proprietăți ale funcțiilor de repartiție și de densitate\nFuncția de repartiție:\n\nFR este mereu pozitivă: \\[F_A(x) \\geq 0\\]\nFR este monoton crescătoare\nFR pornește din 0 și ajunge la valoarea 1: \\[F_A(-\\infty) = 0 \\;\\;\\;\\; F_A(\\infty) = 1\\]\n\nDensitatea de probabilitate / funcția masă de probabilitate:\n\nEste pozitivă: \\[w_A(x) \\geq 0\\]\nIntegrala/suma pe întreg domeniul este 1 \\[\\int_{-\\infty}^\\infty w_A(x) \\mathrm{d}x = 1 \\;\\;\\;\\; \\sum_{x = -\\infty}^\\infty w_A(x) = 1\\]"
  },
  {
    "objectID": "01_SemnaleAleatoare.html#distribuția-normală",
    "href": "01_SemnaleAleatoare.html#distribuția-normală",
    "title": "2  Semnale aleatoare",
    "section": "2.3 Distribuția normală",
    "text": "2.3 Distribuția normală\nÎn cele ce urmează, vom considera numai variabile aleatoare discrete.\nCea mai des întâlnită distribuție în practică este distribuția normală.\nDensitatea de probabilitate are expresia matematică: \\[w_A(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}}\\] și depinde de doi parametri:\n\nmedia \\(\\mu\\) definește “centrul” funcției\ndeviația standard \\(\\sigma\\) controlează “lățimea” funcției\n\n\\(\\sigma\\) mic = funcție îngustă și înaltă\n\\(\\sigma\\) mare = funcție largă și joasă\n\n\nimport matplotlib.pyplot as plt, numpy as np, math;\nmu = 3;\nsigma = 1;\nx = np.linspace(mu-5*sigma,mu+5*sigma,200);\npdf = 1/(sigma*math.sqrt(2*math.pi))*np.exp(-(x-mu)**2/(2*sigma**2)); #**\nplt.plot(x,pdf);\nplt.xlabel('x');\nplt.ylabel('fdp(x)');\nplt.title('The normal distribution $\\mathcal{N}(\\mu=3,\\sigma=1)$');\nplt.savefig('fig/01_RandomSignals_DistributionNormal.png', transparent=True, bbox_inches='tight', dpi=300)\nplt.close()\n\nConstanta de la începutul expresiei asigură normalizarea, adică faptul că probabilitatea totală este 1: \\[\\int_{-\\infty}^\\infty w_A(x) \\mathrm{d}x = 1\\]\nDistribuția normală se notează cu \\(\\mathcal{N}(\\mu, \\sigma^2)\\).\nCând \\(\\mu=0\\) și \\(\\sigma=1\\) avem distribuția normală standard.\nCum “citim” distribuția normală? Fie o variabilă aleatoare \\(A \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) (\\(A\\) ia valori conform distribuției normale cu media \\(\\mu\\) și deviația \\(\\sigma\\)). Putem spune următoarele:\n\n\\(A\\) poate avea orice valoare \\(x \\in \\mathbb{R}\\), întrucât \\(w_A(x) > 0, \\forall x \\in \\mathbb{R}\\) (nici o valoare nu este a priori exclusă);\nValorile cele mai probabile ale lui \\(A\\) sunt în jurul mediei \\(\\mu\\);\nProbabilitatea unor valori \\(x\\) e mai mică cu cât \\(x\\) este mai îndepărtat de centrul \\(\\mu\\), datorită termenului \\(-(x - \\mu)^2\\) de la exponent\n\nDistribuția exprimă așadar o preferință pentru valori apropiate de \\(\\mu\\), cu probabilitate din ce în ce mai scăzută la valori mai depărtate de \\(\\mu\\).\nExemple de valori generate cu distribuția normală standard \\(\\mathcal{N}(\\mu=0, \\sigma^2=1)\\):\nimport matplotlib.pyplot as plt, numpy as np, math;\nmu = 0;\nsigma = 1;\nx = np.linspace(1, 200, 200)\nv = mu + np.sqrt(sigma)*np.random.randn(200)\nplt.plot(x,v)\nplt.ylim(-5,5)\nplt.title('Sample values from the normal distribution');\nplt.savefig('fig/01_RandomSignals_DistributionNormalSampleValues1.png', transparent=True, bbox_inches='tight', dpi=300)\nplt.close()\n\nExemple de valori generate cu distribuția normală \\(\\mathcal{N}(\\mu=2, \\sigma^2=4)\\):\nimport matplotlib.pyplot as plt, numpy as np, math;\nmu = 2;\nsigma = 4;\nx = np.linspace(1, 200, 200)\nv = mu + np.sqrt(sigma)*np.random.randn(200)\nplt.plot(x,v)\nplt.ylim(-5,5)\nplt.title('Sample values from the normal distribution');\nplt.savefig('fig/01_RandomSignals_DistributionNormalSampleValues2.png', transparent=True, bbox_inches='tight', dpi=300)\nplt.close()\n\n\n2.3.1 Distribuția uniformă\nDensitatea de probabilitate uniformă este o funcție constantă între două limite \\(a\\) și \\(b\\):\n\\[w_A(x) =\n\\begin{cases}\n\\frac{1}{b-a}, & x \\in [a, b] \\\\\n0, &\\textrm{elsewhere}\n\\end{cases}\\]\nSe notează cu \\(\\mathcal{U} \\;[a, b]\\).\nimport matplotlib.pyplot as plt, numpy as np, math\na = -1\nb = 3\nx = np.linspace(-2, 4, 60)\npdf = np.hstack( (np.zeros((10)), 1/(b-a)*np.ones((40)),  np.zeros((10))))  #*\nplt.plot(x,pdf)\nplt.xlabel('x')\nplt.ylabel('fdp(x)')\nplt.title('The uniform distribution $\\mathcal{U}\\;[-1,3]$')\nplt.savefig('fig/01_RandomSignals_DistributionUniform.png', transparent=True, bbox_inches='tight', dpi=300)\nplt.close()\n\nCum interpretăm distribuția uniformă?\n\nSunt posibile doar valori din intervalul \\([a, b]\\), restul sunt excluse.\nToate valorile din intervalul \\([a, b]\\) au aceeași șansă\n\n“Înălțimea” funcției este \\(\\frac{1}{b-a}\\) pentru a se asigura normalizarea (aria totală este 1)\n\n\n2.3.2 Alte distribuții\n\nNenumărate variante, apar în diverse aplicații\n\n\n\n2.3.3 Calculul probabilității pentru distribuția normală\nProbabilitatea ca \\(A\\) sa aibă valori între \\(a\\) și \\(b\\) este \\(\\int_a^b w_A(x) dx\\), dar pentru expresia distribuției normale această integrală nu se poate calcula prin formula algebrice (este funcție ne-elementară).\nPentru calcul, se folosesc algoritmi numerici, pe baza funcției de eroare \\(erf(z)\\) (“the error function”): \\[erf(z) = \\frac{2}{\\sqrt{\\pi}} \\int_0^z e^{-t^2} dt\\]\n\n\nValorile funcției erf() sunt tabelate sau se calculează numeric cu algoritmi specializati. De ex., pe Google, căutați \\(erf(0.5)\\)\nFuncția de repartiție a unei distribuții normale oarecare \\(\\mathcal{N}(\\mu, \\sigma^2)\\) se poate calcula ca: \\[F_A(X) = \\frac{1}{2}(1 + erf(\\frac{x - \\mu}{\\sigma \\sqrt{2}}))\\]\nAlte valori folositoare:\n\n\\(erf(-\\infty) = -1\\)\n\\(erf(\\infty) = 1\\)\n\nExercițiu:\n\nFie \\(A\\) o v.a. cu distribuția \\(\\mathcal{N}(3, 2)\\). Calculați probabilitatea ca \\(A \\in [2, 4]\\)\n\nExercițiu:\n\nFie \\(A\\) o v.a. cu distribuția \\(\\mathcal{N}(\\mu, \\sigma)\\).\n\nCalculați probabilitatea ca \\(A\\) să fie la cel mult \\(\\pm\\sigma\\) distanță de valoare medie\nCalculați probabilitatea ca \\(A\\) să fie la cel mult \\(\\pm 2\\sigma\\) distanță de valoare medie\nCalculați probabilitatea ca \\(A\\) să fie la cel mult \\(\\pm 3\\sigma\\) distanță de valoare medie\n\n\n\n\nÎn multe aplicații, anomaliile într-un set de valori se definesc ca fiind valorile aflate la peste \\(3 \\sigma\\) distanță față de medie. De ce oare?\n\n\n2.3.4 Medii statistice\nVariabilele aleatoare sunt caracterizate prin medii statistice (“momente”), care caracterizează densitatea de probabilitate.\n\n2.3.4.1 valoarea medie\nValoarea medie (momentul de ordin 1) a unei variabila aleatoare \\(A\\), notată în mod uzual cu \\(\\mu\\), \\(E\\lbrace A \\rbrace\\) sau \\(\\overline{A}\\), se definește astfel:\n\nPentru v.a. continue: \\[\\overline{A} = E\\{A\\} = \\int_{-\\infty}^{\\infty} x \\cdot w_A(x) dx\\]\nPentru v.a. discrete: \\[\\overline{A} = E\\{A\\} = \\sum_{x=-\\infty}^{\\infty} x \\cdot w_A(x)\\]\n\n\n\nTermenul în limba engleză este “expected value”.\nIn termeni informali, valoarea medie este o sumă în care apar toate valorile posibile \\(x\\) înmulțite cu probabilitatea fiecăreia (pentru cazul continuu, folosim integrala și densitatea de probabilitate, cu aceeași semnificație).\nExemplu: entropia este valoarea medie a informației \\(i(s_n) = -\\log_2{p(s_n)}\\) asociată unui eveniment cu probabilitatea \\(p(s_n)\\): \\[H(X) = \\sum p(s_n) i(s_n)) = - \\sum p(s_n) log_2(p(s_n))\\]\nCe semnificație are, practic, valoarea medie a unei variabile aleatoare? Informal, ea ne dă valoarea numerică centrală în jurul căreia “se învârt” toate valorile pe care le poate lua \\(A\\). Ca interpretare grafică, de multe ori ea este situată “pe la mijlocul” distribuției.\n\nValorile care au probabilitate / densitate de probabilitate ridicată “trag” valoarea medie înspre ele.\nPentru distribuții cu formă simetrică (de ex. distribuția normală, distribuția uniformă), valoarea medie este valoarea centrală a funcției. Ambele laturi ale funcției “trag” valoare medie înspre ele în mod egal, valoarea medie rămâne la mijloc.\nDacă avem de-a face cu un semnal \\(x(t)\\) sau \\(x[n]\\) a cărui valori respectă o distribuție \\(w_A(x)\\), valoarea medie a distribuției va fi componenta continuă a semnalului respectiv.\n\nAlte câteva interpretări:\n\nDacă am avea \\(N \\to \\infty\\) valori aleatoare conform distribuției respective, valoarea medie ar fi media aritmentică a tuturor acestor valori;\nDacă trebuie să prezicem valoarea unei variabile aleatoare \\(X\\), și plătim un cost proporțional cu pătratul erorii pe care o facem, \\((u - X)^2\\), valoarea medie \\(\\mu\\) este cea mai bună alegere, întrucât minimizează costul global: \\[\\mu = \\arg\\min_u \\int_{-\\infty}^{\\infty} (u - x)^2\\cdot w(x) dx\\]\n\nDemonstrație: la tablă: derivare, derivata = 0\n\n\nPentru distribuția normală \\(\\mathcal{N}(\\mu, \\sigma)\\), valoarea medie este chiar parametrul \\(\\mu\\): \\[\\overline{X} = \\mu\\]\nPentru o distribuție uniformă \\(\\mathcal{U} [a, b]\\), valoarea medie este valoarea de la mijlocul intervalului: \\[\\overline{X} = \\frac{a + b}{2}\\]\nCalculul valorii medii este o operație liniară. Pentru două variabile aleatoare independente \\(A\\) și \\(B\\), și oricare două numere reale \\(c_1\\) și \\(c_2\\), avem: \\[E\\{c_1A + c_2B\\} = c_1E\\{A\\} + c_2E\\{B\\}\\]\nSau: \\[E\\{cA\\} = c E\\{A\\}, \\forall c \\in \\mathbb{R}\\] \\[E\\{A + B\\} = E\\{A\\} + E\\{B\\}\\]\nExemplu: Aruncăm trei zaruri \\(A\\), \\(B\\), \\(C\\) și definim \\(D\\) ca suma celor trei valori obținute: \\[D = A+B+C\\]\nCât este valoarea medie a lui \\(D\\)?\n\n\n2.3.4.2 Valoarea pătratică medie\nValoarea pătratică medie (momentul de ordin 2) reprezintă valoarea medie a pătratelor valorilor unei variabile \\(A\\).\nPentru variabile aleatoare continue: \\[\\overline{A^2} = E\\{A^2\\} = \\int_{-\\infty}^{\\infty} x^2 \\cdot w_A(x) dx\\]\nȘi pentru cele discrete: \\[\\overline{A^2} = E\\{A^2\\} = \\sum_{-\\infty}^{\\infty} x^2 \\cdot w_A(x)\\]\nValoarea pătratică medie are o interpretare legată de magnitudinea valorilor posibile. Dacă avem de-a face cu un semnal \\(x(t)\\) sau \\(x[n]\\) a cărui valori sunt generate de o distribuție \\(w_A(x)\\), valoarea pătratică medie a distribuției va fi puterea medie a semnalului respectiv.\n\n\n\n2.3.5 Varianța\nVarianța (momentul centrat de ordin 2) reprezintă valoarea pătratică medie a abaterilor față de valoarea medie.\nPentru variabile aleatoare continue: \\[\\sigma^2 = \\overline{\\left\\{ A - \\mu \\right\\}^2} = \\int_{-\\infty}^{\\infty} (x-\\mu)^2 \\cdot w_A(x) dx\\]\nPentru variabile aleatoare discrete: \\[\\sigma^2 = \\overline{\\left\\{ A - \\mu \\right\\}^2} = \\sum_{-\\infty}^{\\infty} (x-\\mu)^2 \\cdot w_A(x)\\]\nCu alte cuvinte, din valorile variabilei aleatoare \\(A\\) scădem mereu valoarea medie, obținând astfel valori centrate pe \\(0\\) (media lor devine \\(0\\)), și calculăm apoi varianța a ceea ce obținem.\nRădăcina pătrată \\(\\sigma\\) a varianței unei distribuții se numește deviație standard.\nVarianța, sau deviația standard, ne spun cat de mult pot fluctua valorile în jurul valorii medii.\n\nvarianță mică: valorile deviază puțin în jurul mediei, sunt concentrate în jur;\nvarianță mare: valorile deviază mult de la medie.\nDacă avem de-a face cu un semnal \\(x(t)\\) sau \\(x[n]\\) a cărui valori sunt generate de o distribuție \\(w_A(x)\\), varianța va fi puterea medie a componentei alternative a semnalului respectiv (adică puterea medie a semnalului din care am scăzut componenta continuă).\n\n\n2.3.5.1 Legătura între cele trei mărimi statistice\nPentru orice distribuție, cele trei mărimi statistice satisfac relația următoare: \\[\\begin{split}\n\\sigma^2 &= \\overline{\\left\\{ A - \\mu \\right\\}^2} \\\\\n&= \\overline{A^2 - 2 \\cdot A \\cdot \\mu + \\mu^2} \\\\\n&= \\overline{A^2} - 2 \\mu \\overline{A} + \\mu^2 \\\\\n&= \\overline{A^2} - \\mu^2\n\\end{split}\\]\nDacă avem de-a face cu un semnal \\(x(t)\\) sau \\(x[n]\\) a cărui valori urmează o distribuție \\(w_A(x)\\), relația poate fi interpretată în sens energetic:\n\n\\(\\overline{A^2}\\) este puterea totală a semnalului\n\\(\\mu^2\\) este puterea componentei continue\n\\(\\sigma^2\\) este puterea componentei alternative\nputerea totată a semnalului = puterea componentei continue + puterea componentei alternative\n\n\n\n\n2.3.6 Operații cu variabile aleatoare\n\n2.3.6.1 Suma cu o constantă\nFie o variabilă aleatoare \\(A\\), și fie \\(B = 5 + A\\).\n\\(B\\) este tot o variabilă aleatoare, a cărui distribuție \\(w_B(x)\\) este distribuția lui \\(A\\) “translată” cu 5 la dreapta: \\[w_B(x) = w_A(x - 5)\\]\nExemplu:\n\nA este o v.a. cu distribuție normală \\(w_A(x) = \\mathcal{N}(\\mu=3, \\sigma^2=2)\\)\nCare este distribuția variabilei \\(B = 5 + A\\)?\nRăspuns: \\(w_B(x) = \\mathcal{N}(\\mu=8, \\sigma^2=2)\\)\n\n\n\n2.3.6.2 Suma variabilelor aleatoare\nSuma a două sau mai multe variabile aleatoare independente este tot o variabilă aleatoare, a cărei densitate de probabilitate este convoluția densităților termenilor.\nDacă \\(C = A + B\\), atunci: \\[w_C(x) = w_A(x) \\star w_B(x)\\]\nPentru cazul particular cand \\(A\\) și \\(B\\) sunt variabile normale cu \\(\\mathcal{N}(\\mu_A, \\sigma_A^2)\\) și \\(\\mathcal{N}(\\mu_B, \\sigma_B^2)\\), atunci suma lor \\(C\\) are tot o distribuție normală \\(\\mathcal{N}(\\mu_C, \\sigma_C^2)\\), având media lui \\(C\\) egală cu suma mediilor lui \\(A\\) și \\(B\\): \\[\\mu_C = \\mu_A + \\mu_B\\] iar varianța egală cu suma varianțelor: \\[\\sigma_C^2 = \\sigma_A^2 + \\sigma_B^2\\]\n\n\n\n2.3.7 Alte transformări\nÎn general, o funcție aplicată unei variabile aleatoare produce o altă variabilă aleatoare, cu altă distribuție, posibil cu totul diferită de cea inițială.\nExemple: dacă \\(A\\) este o v.a. distribuită \\(\\mathcal{U}\\;[0,10]\\), atunci\n\n\\(B = 5 + A\\) este o altă v.a., distribuită \\(\\mathcal{U}\\;[5,15]\\)\n\\(C = A^2\\) este de asemenea o v.a.\n\\(D = cos(A)\\) este de asemenea o v.a.\n\nVariabilele A, B, C, D nu sunt însă independente, întrucât o anumită valoare a uneia implică automat și valoarea celorlalte."
  },
  {
    "objectID": "01_SemnaleAleatoare.html#sisteme-de-mai-multe-variabile-aleatoare",
    "href": "01_SemnaleAleatoare.html#sisteme-de-mai-multe-variabile-aleatoare",
    "title": "2  Semnale aleatoare",
    "section": "2.4 Sisteme de mai multe variabile aleatoare",
    "text": "2.4 Sisteme de mai multe variabile aleatoare\nFie un sistem cu două variabile aleatoare continue, \\(A\\) și \\(B\\).\nCare este probabilitatea ca \\(A\\) să ia valori în jurul lui \\(x\\) ȘI B să ia valori în jurul lui \\(y\\)?\n\n\nAruncând două zaruri \\(A\\) și \\(B\\), care este probabilitatea ca perechea \\((A,B)\\) să aibă valoarea (6, 4)?\nDistribuția valorilor perechii \\((A,B)\\) este descrisă de o densitatea de probabilitate comună \\(w_{AB}(x,y)\\), și de o funcția de repartiție comună \\(F_{AB}(x,y)\\).\nFuncția de repartiție comună se definește ca: \\[F_{AB}(x,y) = P_{AB}\\left\\{ A \\leq x \\cap B \\leq y \\right\\}\\]\nDensitatea de probabilitate comună: \\[w_{AB}(x,y) = \\frac{\\partial^2 F_{AB}(x,y)}{\\partial x \\partial y}\\]\nDensitatea de probabiliate comună ne dă probabilitatea ca perechea \\((A,B)\\) să aibă valoarea într-o vecinătate a \\((x,y)\\)\nCele două funcții se definesc similar și pentru variabile discrete, înlocuind integralele cu sume: \\[w_{AB}(x,y) = P\\left\\{ A = x \\cap B = y \\right\\}\\]\n\n2.4.1 Variabile independente\nDouă variabile \\(A\\) și \\(B\\) sunt independente dacă se respectă relația: \\[w_{AB}(x,y) = w_A(x) \\cdot w_B(y)\\]\nCa interpretare, acest lucru înseamnă că valoarea uneia nu influențează în nici un fel valoarea celeilalte.\nRelația este valabilă și pentru funcția de repartiție, atât pentru variabile continue cât și pentru discrete, și se extinde similar pentru cazul în care sunt mai mult de două variabile.\nExercițiu:\n\nCalculați probabilitatea ca trei v.a. \\(X\\), \\(Y\\) și \\(Z\\) i.i.d. \\(\\mathcal{N}(-1,1)\\) să fie toate pozitive\n\ni.i.d = “independente și identic distribuite”\n\n\n\n\n2.4.2 Distribuția normală multivariată\nDistribuția normală se poate extinde în mod natural la vectori cu dimensiune \\(N\\).\nPentru un vector \\(\\mathbf{x}\\) de dimensiune N \\[\\mathbf{x} = [x_1, x_2, ... x_N]\\] distribuția se definește ca: \\[w_A(\\mathbf{x})= \\frac{1}{\\sqrt{(2\\pi)^N |\\Sigma|}}\ne^{-\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu})^T\\Sigma^{-1}(\\mathbf{x} - \\boldsymbol{\\mu})}\\]\nFuncția are doi parametri matriciali:\n\nmedia \\(\\boldsymbol{\\mu} = [\\mu_1, \\mu_2, ... \\mu_N]\\) este un vector de dimensiune \\(N\\) care definește “centrul” funcției;\nmatricea de covarianță \\(\\Sigma\\) care definește orientarea și forma eliptică a funcției.\n\nDistribuția normală multivariată are forma de tipul unui “munte” elipsoid, pentru care media \\(\\boldsymbol{\\mu}\\) reprezintă coordonatele vârfului central. Matricea de covarianță \\(\\Sigma\\) determina forma “muntelui”, în felul următor:\n\nMatrice are \\(N\\) vectori proprii \\(\\mathbf{e_i}\\) care dau direcțiile ortogonale ale axelor care definesc elipsa;\nCele \\(N\\) valori proprii \\(\\lambda_i\\) controlează extinderea elipsei de-a lungul fiecărei axe.\n\nGRAFICE, GRAFICE, GRAFICE\n\n2.4.2.1 Caz particular: componente independente\nÎn cazul în care matricea de covarianță este o matrice diagonală:\n\\[\\Sigma =\n\\begin{bmatrix}\n\\sigma^2_1 & 0 & 0 & \\cdots & 0 \\\\\n0 & \\sigma^2_2 & 0 & \\cdots & 0 \\\\\n0 & 0 & \\sigma^2_3 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & \\sigma^2_N \\\\\n\\end{bmatrix}\\]\ndistribuția multiavariată se descompune ca un produs de distribuții individuale, ceea ce reflectă faptul că componentele vectorului \\(\\mathbf{x}\\) sunt independente:\n\\[\\begin{split}\nw_A(\\mathbf{x}) &=\n\\frac{1}{\\sigma_1 \\cdot ... \\cdot \\sigma_N \\sqrt{(2\\pi)^N}}\ne^{(-\\frac{(\\mathbf{x_1} - \\boldsymbol{\\mu_1})^2}{2\\sigma^2_1}) \\cdot (-\\frac{(\\mathbf{x_2} - \\boldsymbol{\\mu_2})^2}{2\\sigma^2_2}) \\cdot \\dots (-\\frac{(\\mathbf{x_N} - \\boldsymbol{\\mu_N})^2}{2\\sigma^2_N})}  \\\\\n&=\\frac{1}{\\sigma_1 \\sqrt{(2\\pi)}} e^{-\\frac{(\\mathbf{x_1} - \\boldsymbol{\\mu_1})^2}{2\\sigma^2_1}}\n\\cdot\n\\frac{1}{\\sigma_2 \\sqrt{(2\\pi)}} e^{-\\frac{(\\mathbf{x_2} - \\boldsymbol{\\mu_2})^2}{2\\sigma^2_2}}\n\\cdot \\dots \\cdot\n\\frac{1}{\\sigma_N \\sqrt{(2\\pi)}} e^{-\\frac{(\\mathbf{x_N} - \\boldsymbol{\\mu_N})^2}{2\\sigma^2_N}} \\\\\n&=w_1(x_1) \\cdot w_2(x_2) \\cdot \\dots \\cdot w_N(x_N)\n\\end{split}\\]\nAcest lucru înseamnă, practic, că fiecare componentă \\(x_k\\) din vectorul \\(\\mathbf{x}\\) este independentă de toate celelalte și respectă o distribuție normală cu media \\(\\mu_k\\) și varianța \\(\\sigma^2_k\\). Cu alte cuvinte:\n\\[\\mathcal{N}(\\boldsymbol{\\mu}, \\Sigma) = \\mathcal{N}(\\mu_1, \\sigma_1) \\cdot \\mathcal{N}(\\mu_2, \\sigma_2) \\cdot \\dots \\cdot \\mathcal{N}(\\mu_N, \\sigma_N)\\]\n\n\n2.4.2.2 Caz particular: distribuție izotrop\nÎn cazul în care matricea de covarianță este de forma unei matrici matricea unitate, având aceeași valoare \\(\\sigma\\) pe toate liniile: \\[\\Sigma = \\sigma^2 \\mathbf{I}_N\\]\navem o distribuție normală multivariată izotropă:\n\n\nTermenul “izotrop” înseamnă “cu aceleași proprietăți în orice direcție” și se referă la forma circulară a funcției.\n\\[\\begin{split}\nw_A(\\mathbf{x})= &\\frac{1}{\\sigma^N \\sqrt{(2\\pi)^N}}\ne^{-\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu})^T(\\mathbf{x} - \\boldsymbol{\\mu})} \\\\\n&=\n\\end{split}\\]\nÎn acest caz, toata componentele \\(x_k\\) sunt independente și respectă fiecare o distribuție normală cu medie \\(\\mu_k\\), dar cu aceeași varianță \\(\\sigma^2\\).\nSe observă că în acest caz valoarea densității de probabilitate într-un punct \\(\\mathbf{x}=[x_1,...x_N]\\) depinde de distanța de la punctul respectiv la punctul central \\(\\boldsymbol{\\mu} = [\\mu_1,...\\mu_N]\\).\nDistanța Euclideană (geometrică) între 2 vectori N-dimensionali se definește astfel:\n\\[d(\\mathbf{u},\\mathbf{v}) = \\| \\mathbf{u}-\\mathbf{v} \\| = \\sqrt{(u_1-v_1)^2+...+(u_N - v_N)^2}\\]\nCazuri particulare în funcție de dimensiunea \\(N\\):\n\nUnidimensional: \\(\\|\\mathbf{u}-\\mathbf{v}\\| = |u-v|\\)\n2D: \\(\\|\\mathbf{u}-\\mathbf{v}\\| = \\sqrt{(u_1 - v_1)^2 + (u_2 - v_2)^2}\\)\n3D: \\(\\|\\mathbf{u}-\\mathbf{v}\\| = \\sqrt{(u_1 - v_1)^2 + (u_2 - v_2)^2 + (u_3 - v_3)^2}\\)\n…\nN-dimensional: \\(\\|\\mathbf{u}-\\mathbf{v}\\| = \\sqrt{\\sum_{i=1}^N(u_i - v_i)^2}\\)\n\nAceeași relație se poate extinde și pentru semnale continue, văzute ca vectori cu o infinitate de puncte unul lângă altul:\nÎn cazul unei distribuții normale multivariate izotrope, densitatea de probabilitate într-un punct depinde de pătratul distanței Euclidiene față de media \\(\\boldsymbol{\\mu} = [\\mu_1,...\\mu_N]\\)\n\nValoarea maximă este chiar in punctul \\(\\boldsymbol{\\mu}\\)\nPuncte aproapiate de \\(\\mu\\) au probabilitate mai mare\nPuncte mai depărtate de \\(\\mu\\) au probabilitate mai redusă\nDouă puncte la aceeași distanță de \\(\\mu\\) au aceeași probabilitate\n\n\n\n\n2.4.3 Distribuția normală 2D\n\nDistribuția a 2 v.a. normale (distribuția normală 2D) \n\n\n\n2.4.4 Distribuția normală 2D - vedere de sus\n\nVedere de sus\nAici, \\(\\mu = (0,0)\\)\nProbabilitatea scade pe măsură ce crește distanța față de centru, în cercuri concentrice (simetric)"
  },
  {
    "objectID": "01_SemnaleAleatoare.html#ii.2-procese-aleatoare",
    "href": "01_SemnaleAleatoare.html#ii.2-procese-aleatoare",
    "title": "2  Semnale aleatoare",
    "section": "2.5 II.2 Procese aleatoare",
    "text": "2.5 II.2 Procese aleatoare\n\n2.5.1 Procese aleatoare\n\nUn proces aleator = o secvență de variabile aleatoare indexate (înșiruite) în timp\nProces aleator în timp discret \\(f[n]\\) = o secvență de v.a. la momente de timp discrete\n\nex: o secvență de 50 aruncări de zar, cotația zilnică a unor acțiuni la bursă\n\nProces aleator în timp continuu \\(f(t)\\) = o secvență de v.a. la orice moment de timp\n\nex: un semnal tip zgomot de tensiune\n\nFiecare eșantion dintr-un proces aleator este o v.a. de sine stătătoare\n\nex:. \\(f(t_0)\\) = valoarea la momentul \\(t_0\\) este o v.a.\n\n\n\n\n2.5.2 Realizări ale proceselor aleatoare\n\nRealizare a unui p.a. = o secvență particulară de realizări ale v.a. componente\n\nex: un anume semnal de zgomot măsurat cu un osciloscop; am obținut o anume realizare, dar am fi putut obține orice altă realizare\n\nNotația uzuală: \\(f^{(k)}[n]\\) sau \\(f^{(k)}(t)\\)\n\n\\(k\\) indică realizarea particulară care se consideră\n\\(t\\) sau \\(n\\) este timpul\n\nCând considerăm un p.a., considerăm întregul set de realizări posibile\n\nla fel ca atunci când considerăm o v.a.\n\n\n\n\n2.5.3 Proces aleator = un fenomen 2-D\n\nUn proces aleator trebuie vizualizat în două dimensiuni:\n\n\\(f^{(k)}[n]\\) sau \\(f^{(k)}(t)\\) depind de două variabile:\n\n\\(k\\) = realizarea\n\\(t\\) sau \\(n\\) = timpul\n\n\n\n\n\n2.5.4 Proces aleator = un fenomen 2-D\n\n\nsursa: “Information-Based Inversion and Processing with Applications” Edited by Tadeusz J. Ulrych, Mauricio D. Sacchi, Volume 36,\n\n\n\n2.5.5 Proces aleator = un fenomen 2-D\n\n\nsursa: Razdolsky, L. (2014). Random Processes. In Probability-Based Structural Fire Load (pp. 89-136). Cambridge: Cambridge University Press\n\n\n\n2.5.6 Proces aleator = un fenomen 2-D\n\n\nsursa: https://www.quora.com/What-is-the-difference-between-a-stationary-ergodic-and-a-stationary-non-ergodic-process\n\n\n\n2.5.7 Două feluri de valori medii\n\nProcesele aleatoare au două tipuri de valori medii:\n\nValori medii statistice = calculate la un timp \\(t\\) sau \\(n\\) fixat, de-a lungul tuturor realizărilor posibile\nValori medii temporale = calculate pentru o realizare \\(k\\) fixată, de-a lungul timpului\n\n\n\n\n2.5.8 Două feluri de valori medii\n\n\nsursa: https://www.quora.com/What-is-the-difference-between-a-stationary-ergodic-and-a-stationary-non-ergodic-process\n\n\n\n2.5.9 Distribuții de ordin 1 ale proceselor aleatoare\n\nFiecare eșantion \\(f(t_1)\\) dintr-un proces aleator este o v.a.\n\ndescris de o distribuție de ordin 1\nare FR \\(F_1(x;t_1)\\)\nare FDP / FMP \\(w_1(x;t_1) = \\frac{dF_1(x;t_1)}{dx}\\)\ndistribuția depinde de momentul \\(t_1\\)\n\nUn eșantion la alt moment \\(t_2\\) este o v.a. diferită, cu funcții posibil diferite\n\naltă FR \\(F_1(x;t_2)\\)\naltă FDP / FMP \\(w_2(x;t_2) = \\frac{dF_1(x;t_2)}{dx}\\)\n\nAceste funcții descriu distribuția valorilor unui eșantion\nIndicele \\(w_1\\) arată că considerăm o singură v.a. din proces (distribuții de ordin 1-)\nSimilar pentru p.a. discrete\n\n\n\n2.5.10 Distribuții de ordin 2\n\nO pereche de v.a. \\(f(t_1)\\) și \\(f(t_2)\\) formează un sistem de 2 v.a.:\n\nsunt descrise de o distribuție de ordin 2\nau FR comună \\(F_2(x_i, x_j; t_1, t_2)\\)\nau FDP / FMP comună \\(w_2(x_i, x_j; t_1, t_2) = \\frac{\\partial^2 F_2(x_i, x_j;t_1, t_2)}{\\partial x_i \\partial x_j}\\)\ndistribuția depinde de momentele \\(t_1\\) și \\(t_2\\)\n\nAceste funcții descriu cum sunt distribuite valorile perechilor (distribuții de ordin 2)\nSimilar pentru p.a. discrete\n\n\n\n2.5.11 Distribuții de ordin n\n\nGeneralizare la un grup de \\(n\\) eșantioane dintr-un p.a.\nUn set de \\(n\\) v.a. \\(f(t_1), ...f(t_n)\\) dintr-un proces aleator \\(f(t)\\):\n\nsunt descrise de o distribuție de ordin n\nau FR comună \\(F_n(x_1,... x_n; t_1,... t_n)\\)\nau FDP / FMP comună \\(w_n(x_1,... x_n; t_1,... t_n) = \\frac{\\partial^2 F_n(x_1,... x_n;t_1,... t_n)}{\\partial x_1 ... \\partial x_n}\\)\ndepind de momentele de timp \\(t_1\\), \\(t_2\\), … \\(t_n\\)\n\nAceste funcții descriu cum sunt distribuite valorile seturilor de \\(n\\) eșantioane (distribuții de ordin n)\nSimilar pentru p.a. discrete\n\n\n\n2.5.12 Medii statistice\nProcesele aleatoare sunt caracterizate de medii statistice și temporale\nPentru procese continue:\n\nValoarea medie \\[\\overline{f(t_1)} = \\mu(t_1) = \\int_{-\\infty}^{\\infty} x \\cdot w_1(x; t_1) dx\\]\nValoarea pătratică medie \\[\\overline{f^2(t_1)} = \\int_{-\\infty}^{\\infty} x^2 \\cdot w_1(x; t_1) dx\\]\n\n\n\n2.5.13 Medii statistice - varianța\n\nVarianța \\[\\sigma^2(t_1) = \\overline{\\left\\{ f(t_1) - \\mu(t_1) \\right\\}^2} = \\int_{-\\infty}^{\\infty} (x-\\mu(t_1)^2 \\cdot w_1(x; t_1) dx\\]\n\n\nLegătura între aceste trei mărimi: \\[\\begin{split}\n  \\sigma^2(t_1) =& \\overline{\\left\\{ f(t_1) - \\mu(t_1) \\right\\}^2} \\\\\n  =& \\overline{f(t_1)^2 - 2f(t_1)\\mu(t_1) + \\mu(t_1)^2} \\\\\n  =& \\overline{f^2(t_1)} - \\mu(t_1)^2\n  \\end{split}\\]\nObservații:\n\naceste trei valori sunt calculate pentru toate realizările, la momentul \\(t_1\\)\nele caracterizează doar eșantionul de la momentul \\(t_1\\)\nla alt moment de timp \\(t_2\\), v.a. \\(f(t_2)\\) este diferită, și valorile medii pot diferi\n\n\n\n\n2.5.14 Medii statistice - autocorelația\nMedii statistice care caracterizează o pereche de eșantioane:\n\nFuncția de autocorelație \\[R_{ff}(t_1,t_2) = \\overline{f(t_1) f(t_2)} = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty x_1 x_2 w_2(x_1, x_2; t_1, t_2) dx_1 dx_2\\]\nFuncția de corelație (pentru două procese aleatoare diferite \\(f(t)\\) și \\(g(t)\\)) \\[R_{fg}(t_1,t_2) = \\overline{f(t_1) g(t_2)} = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty x_1 y_2 w_2(x_1, y_2; t_1, t_2) dx_1 dy_2\\]\n\n\nObservații:\n\naceste funcții pot fi diferite pentru perechi de valori luate la momente diferite (\\(t_1\\),\\(t_2\\))\n\n\n\n\n2.5.15 Procese aleatoare discrete\nPentru procese aleatoare discrete, se înlocuiește \\(\\int\\) cu \\(\\sum\\), și notația \\(f(t)\\) cu \\(f[t]\\):\n\n\\(\\overline{f[t_1]} = \\mu(t_1) = \\sum_{x=-\\infty}^{\\infty} x \\cdot w_1(x; t_1)\\)\n\\(\\overline{f^2[t_1]} = \\sum_{x=-\\infty}^{\\infty} x^2 \\cdot w_1(x; t_1)\\)\n\\(\\sigma^2(t_1) = \\overline{\\left\\{ f[t_1] - \\mu(t_1) \\right\\}^2} = \\sum_{x=-\\infty}^{\\infty} (x-\\mu(t_1)^2 \\cdot w_1(x; t_1)\\)\n\\(R_{ff}(t_1,t_2) = \\overline{f[t_1] f[t_2]} = \\sum_{x_1=-\\infty}^\\infty \\sum_{x_2=-\\infty}^\\infty x_1 x_2 w_2(x_1, x_2; t_1, t_2)\\)\n\\(R_{fg}(t_1,t_2) = \\overline{f[t_1] g[t_2]} = \\sum_{x_1=-\\infty}^\\infty \\sum_{x_2=-\\infty}^\\infty x_1 y_2 w_2(x_1, y_2; t_1, t_2)\\)\n\n\n\n2.5.16 Medii temporale\n\nDacă avem acces doar la o singură realizare \\(f^{(k)}(t)\\) a procesului?\nCalculăm valorile medii pentru o singură realizare \\(f^{(k)(t)}\\), de-a lungul timpului\n\n\n\n2.5.17 Medii temporale\nMedii temporale pentru procese aleatoare continue:\n\nValoarea medie temporală \\[\\overline{f^{(k)}(t)} = \\mu^{(k)} = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{T/2}^{T/2} f^{(k)}(t) dt\\]\nValoarea medie pătratică temporală \\[\\overline{[f^{(k)}(t)]^2} = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{-T/2}^{T/2} [f^{(k)}(t)]^2 dt\\]\n\n\n\n2.5.18 Varianța temporală\n\nVarianța temporală \\[\\sigma^2 = \\overline{\\left\\{ f^{(k)}(t) - \\mu^{(k)} \\right\\}^2} = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{-T/2}^{T/2} (f^{(k)}(t)-\\mu^{(k)})^2 dt\\]\n\n\nRelația dintre cele trei mărimi: \\[\\sigma^2 = \\overline{[f^{(k)}(t)]^2} - [\\mu^{(k)}]^2\\]\nObservație:\n\naceste valori nu mai depind de timpul \\(t\\)\n\n\n\n\n2.5.19 Autocorelația temporală\n\nFuncția de autocoreație temporală \\[\\begin{split}\nR_{ff}(t_1,t_2) =& \\overline{f^{(k)}(t_1 + t) f^{(k)}(t_2+t)} \\\\\n=& \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{-T/2}^{T/2} f^{(k)}(t_1+t) f^{(k)}(t_2 + t) dt\n\\end{split}\\]\nFuncția de corelație temporală (pentru două procese diferite \\(f(t)\\) și \\(g(t)\\)) \\[\\begin{split}\nR_{fg}(t_1,t_2) =& \\overline{f^{(k)}(t_1 + t) g^{(k)}(t_2+t)}\\\\\n=& \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{-T/2}^{T/2} f^{(k)}(t_1+t) g^{(k)}(t_2 + t) dt\n\\end{split}\\]\n\n\n\n2.5.20 Procese aleatoare discrete\nPentru procese aleatoare discrete, se înlocuiește \\(\\int\\) cu \\(\\sum\\), \\(T\\) cu \\(N\\), și se împarte la \\(2N+1\\) în loc de \\(2T\\)\n\n\\(\\overline{f^{(k)}[t]} = \\mu^{(k)} = \\lim_{N \\to \\infty} \\frac{1}{2N+1} \\sum_{t=-N}^{N} f^{(k)}[t]\\)\n\\(\\overline{[f^{(k)}[t]]^2} = \\lim_{N \\to \\infty} \\frac{1}{2N+1} \\sum_{t=-N}^{N} (f^{(k)}[t])^2\\)\n\\(\\sigma^2 = \\overline{\\left\\{ f^{(k)}[t] - \\mu^{[k]} \\right\\}^2} = \\lim_{N \\to \\infty} \\frac{1}{2N+1} \\sum_{t=-N}^{N} (f^{(k)}[t]-\\mu^{(k)})^2\\)\n\n\n\n2.5.21 Procese aleatoare discrete\n\nAutocorelația temporală:\n\n\\[\\begin{split}\nR_{ff}(t_1,t_2) =& \\overline{f^{(k)}[t_1 + t] f^{(k)}[t_2+t]} \\\\\n=& \\lim_{N \\to \\infty} \\frac{1}{2N+1} \\sum_{t=-N}^{N} f^{(k)}[t_1+t] f^{(k)}[t_2 + t]\n\\end{split}\\]\n\nCorelația temporală:\n\n\\[\\begin{split}\nR_{fg}(t_1,t_2) =& \\overline{f^{(k)}[t_1 + t] g^{(k)}[t_2+t]}\\\\\n=& \\lim_{N \\to \\infty} \\frac{1}{2N+1} \\sum_{t=-N}^{N} f^{(k)}[t_1+t] g^{(k)}[t_2 + t]\n\\end{split}\\]\n\n\n2.5.22 Realizări de lungime finită\n\nE posibil să avem o realizare de lungime finită (de ex. un vector cu 1000 de eșantioane dintr-o realizare a unui p.a.)\nCum calculăm mediile temporale?\nSe fac sumele/integralele de la \\(\\int_{t_{min}}^{t_{max}}\\) sau \\(\\sum_{t_{min}}^{t_{max}}\\) în loc de la \\(-\\infty\\) la \\(\\infty\\)\nExemplu: calculați mediile temporale pentru realizarea de lungime finită \\[\\{1,-1,2,-2,3,-3,4,-4,5,-5\\}\\]\n\n\n\n2.5.23 Medii statistice și temporale\n\nMediile statistice sunt, de obicei, cele de dorit\n\ndar necesită cunoașterea distribuțiilor \\(w(x)\\), care în practică sunt rareori cunoscute\n\nÎn practică, de obicei avem acces doar la o singură realizare, obținută printr-o măsurătoare\n\ndeci putem calcula doar mediile temporale pe acea realizare\n\nDin fericire, în multe cazuri mediile statistice și temporale sunt identice (“ergodicitate”)\n\n\n\n2.5.24 Procese aleatoare staționare\n\nPână acum, am considerat că mediile statistice depind de timp\n\npot fi diferite pentru un eșantion de la \\(t_1\\) și de la \\(t_2\\)\n\nProces aleator staționar = dacă mediile statistice rămân aceleași la modificarea originii timpului (întârzierea semnalului)\nAltfel spus: distribuțiile (FDP/FMP) eșantioanelor rămân identice la modificarea originii timpului \\[w_n(x_1,...x_n; t_1,...t_n) = w_n(x_1,...x_n; t_1+\\tau,... t_n + =tau)\\]\nPractic, pentru a fi staționar trebuie ca toate mediile statistice să nu mai depindă de timp \\(t\\)\n\n\n\n2.5.25 Staționar în sens strict sau larg\n\nProces aleator staționar în sens strict:\n\nrelația e valabilă pentru distribuțiile de orice ordin \\(n\\)\nvaloarea medie, valoarea pătratică medie, varianța, autocorelația și toate celelalte statistici de ordin superior nu depind de originea timpului \\(t\\)\n\nProces aleator staționar în sens larg:\n\nrelația e valabilă doar pentru distribuțiile de ordin \\(n=1\\) și \\(n=2\\) (distribuțiile unui singur eșantion, sau a două eșantioane)\ndoar valoarea medie, valoarea pătratică medie, varianța și autocorelația nu depind de originea timpului \\(t\\), dar statisticile de ordin superior pot depinde\n\n\n\n\n2.5.26 Procese aleatoare staționare\n\nEste procesul aleator schițat mai jos staționar sau nu?\n\n\n\nsursa: SEX, LIES & STATISTICS, Ned Wright, http://www.astro.ucla.edu/~wright/statistics/\n\n\n\n2.5.27 Procese aleatoare staționare\n\nRăspuns: ne-staționar\nSe observă că varianța nu este aceeași la toate momentele de timp\n\n\n\n2.5.28 Consecințe ale staționarității\n\nPentru distribuții ale unui singur eșantion (de ordin \\(n=1\\)): \\[w_1(x_i;t_1) = w_1(x_i; t_2) = w_1(x_i)\\]\nValoarea medie, valoarea medie pătratică, varianța unui eșantion sunt identice la orice moment de timp \\(t\\) \\[\\overline{f(t)} = constant, \\forall t\\] \\[\\overline{f^2(t)} = constant, \\forall t\\] \\[\\sigma^2(t) = constant, \\forall t\\]\n\n\n\n2.5.29 Consecințe ale staționarității\n\nPentru distribuții ale unor perechi de eșantioane (de ordin \\(n=2\\)): \\[w_2(x_i,x_j;t_1,t_2) = w_2(x_i,x_j;0, t_2-t_1) = w_2(x_i,x_2; t_2-t_1)\\]\nFuncția de autocorelație depinde doar de diferența de timp \\(\\tau = t_2 - t_1\\) dintre eșantioane \\[R_{ff}(t_1,t_2) = R_{ff}(0, t_2 - t_1) = R_{ff}(\\tau) = \\overline{f(t) f(t + \\tau)}\\]\nDepinde doar de valoarea \\(\\tau = t_2 - t_1\\) = diferența de timp dintre cele două eșantioane\n\n\n\n2.5.30 Consecințe ale staționarității\nDefiniția funcției de autocorelație pentru p.a. staționare:\n\nAutocorelația statistică: formula rămâne aceeași\nAutocorelația temporală:\n\npentru p.a. continue \\[\\begin{split}\n  R_{ff}(\\tau) = \\overline{f(t) f(t + \\tau)} = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{-T/2}^{T/2} f^{(k)}(t) f^{(k)}(t + \\tau) dt\n  \\end{split}\\]\npentru p.a. discrete \\[\\begin{split}\n  R_{ff}(\\tau) = \\overline{f(t) f(t + \\tau)} = \\lim_{N \\to \\infty} \\frac{1}{2N+1} \\sum_{t=-N}^{N} f^{(k)}[t] f^{(k)}[t + \\tau]\n  \\end{split}\\]\nlungime finită: se limitează integralele / sumele la intervalul avut la dispoziție, \\(\\int_{t_{min}}^{t_{max}}\\) sau \\(\\sum_{t_{min}}^{t_{max}}\\)\n\n\n\n\n2.5.31 Consecințe ale staționarității\n\nPentru funcția de corelație, definiția este similară cu cea de la autocorelație de mai sus\nCorelația depinde doar de diferența de timp \\(\\tau = t_2 - t_1\\) dintre cele două eșantioane \\[R_{fg}(t_1,t_2) = R_{fg}(0, t_2 - t_1) = R_{fg}(\\tau) = \\overline{f(t) g(t + \\tau)}\\]\n\n\n\n2.5.32 Interpretarea autocorelației\n\n\\(R_{ff}(\\tau) = \\overline{f(t) f(t + \\tau)}\\) = media produsului a două eșantioane situate la distanță de \\(\\tau\\)\n\nne spune dacă eșantioanele variază la fel sau nu\n\nIdem pentru corelație, doar că eșantioanele provin din p.a. diferite, \\(f\\) și \\(g\\)\n\n\n\n2.5.33 Interpretarea autocorelației\n\nExemple:\n\n\\(R_{ff}(0.5) > 0\\): două eșantioane decalate cu \\(0.5\\) secunde tind să varieze în aceeași direcție (ambele pozitive, ambele negative => produsele sunt majoritar pozitive)\n\ndacă se cunoaște una dintre ele, se poate “ghici” ceva despre cealaltă\n\n\\(R_{ff}(1) < 0\\): două eșantioane decalate cu 1 secundă tind să varieze în direcții opuse (când unul e pozitiv, celălalt e negativ => produsele sunt majoritar negative)\n\ndacă se cunoaște una dintre ele, se poate “ghici” ceva despre cealaltă\n\n\\(R_{ff}(2) = 0\\): două eșantioane decalate cu 2 secunde sunt necorelate (produsele sunt în medie 0, deci cele două eșantioane au la fel de multe șanse de a fi de același semn sau cu semne contrare)\n\ndacă se cunoaște una dintre ele, nu se mai poate “ghici” ceva despre cealaltă\n\n\n\n\n\n2.5.34 Procese aleatoare ergodice\n\nÎn practică, avem acces la o singură realizare\nProces aleator ergodic = dacă mediile temporale pe orice realizare sunt identice cu mediile statistice\nErgodicitatea înseamnă:\n\nSe pot calcula toate mediile pe baza unei singure realizări (oricare)\n\ndar realizarea respectivă trebuie să fie foarte lungă (lungimea \\(\\to \\infty\\)) pentru valori precise\n\nToate realizările sunt similare unele cu altele, dpdv statistic\n\no realizare este caracteristică pentru întreg procesul aleator\n\n\n\n\n\n2.5.35 Procese aleatoare ergodice\n\nMajoritatea proceselor aleatoare de interes sunt ergodice și staționare\n\nde ex. zgomote de tensiune\n\nExemplu de proces aleator ne-ergodic:\n\nse aruncă un zar, următoarele 50 valori sunt identice cu prima valoare\n\no singură realizare nu e caracteristică pentru tot procesul\n\n\n\n\n\n2.5.36 Procese aleatoare ergodice\n\n\nXKCD 221 (link aici: https://xkcd.com/221/)\nSe consideră toate numerele care s-ar fi putut obține în loc de 4 (1,2,3,5 sau 6)\nCare e problema aici?\n\nproces aleator staționar sau ne-staționar?\nproces aleator ergodic sau ne-ergodic?"
  },
  {
    "objectID": "01_SemnaleAleatoare.html#i.3-proprietăți-ale-autocorelației",
    "href": "01_SemnaleAleatoare.html#i.3-proprietăți-ale-autocorelației",
    "title": "2  Semnale aleatoare",
    "section": "2.6 I.3 Proprietăți ale autocorelației",
    "text": "2.6 I.3 Proprietăți ale autocorelației\n\n2.6.1 Densitatea spectrală de putere\n\nDensitatea spectrală de putere (DSP) \\(S_{ff}(\\omega)\\) reprezintă puterea unui semnal în funcție de frecvență (\\(f\\) sau \\(\\omega = 2 \\pi f\\))\nPentru un semnal determinist (ne-aleator), este dată de modului transf. Fourier la pătrat: \\[S_{ff}(\\omega) = |F(\\omega)|^2\\]\nPuterea în banda de frecvență \\([f_1, f_2]\\) este \\(\\int_{f_1}^{f_2} S_{ff}(\\omega) d\\omega\\)\nPuterea totală a procesului aleator este \\(P = \\int_{-\\infty}^{\\infty} S_{ff}(\\omega) d\\omega\\)\nDSP este o funcție măsurabilă practic:\n\npoate fi determinată experimental\neste importantă în aplicații practice (inginerești)\n\n\n\n\n2.6.2 Densitatea spectrală de putere\n\nCe reprezintă DSP pentru un proces aleator?\n\nnu mai avem un singur semnal, cu o infinitate de realizări posibile\nfiecare realizare are o transformată Fourier proprie, diferită\nDSP fiind diferită pentru fiecare realizare în parte, este, ea însăși, un proces aleator\n\nDSP a unui proces aleator = media DSP pentru toate realizările posibile\nAre aceeași utilitate și semnificație ca în cazul unui semnal determinist, doar că în medie în raport cu toate realizările posibile\n\npentru o realizare particulară, DSP poate varia în jurul DSP medii\n\n\n\n\n2.6.3 Teorema Wiener-Hincin\nTeorema Wiener-Hincin:\n\nDensitatea spectrală de putere = transformata Fourier a funcției de autocorelație \\[S_{ff}(\\omega) = \\int_{-\\infty}^{\\infty} R_{ff}(\\tau) e^{- j \\omega \\tau} d\\tau\\] \\[R_{ff}(\\tau) = \\frac{1}{2 \\pi}\\int_{-\\infty}^{\\infty} S_{ff}(\\omega) e^{j \\omega \\tau} d\\omega\\]\nFără demonstrație\nLeagă două concepte de natură diferită\n\nFuncția de autocorelație: o proprietate statistică\nDSP: o proprietate fizică (ține de energia semnalului; importantă în aplicații practice)\n\n\n\n\n2.6.4 Zgomot alb\n\nZgomot alb = un proces aleator cu funcția de autocorelație egală cu un Dirac \\[R_{ff}(\\tau) = \\delta(\\tau)\\]\n\neste proces aleator: orice eșantion este o variabilă aleatoare\nautocorelația este un Dirac: este 0 pentru orice \\(\\tau \\neq 0\\)\noricare două eșantioane diferite (\\(\\tau \\neq 0\\)) au corelație zero (necorelate)\n\nvalorile a două eșantioane distincte nu au legătură între ele\n\n\n\n\n\n2.6.5 Zgomot alb\n\nDensitatea spectrală de putere = transf. Fourier a unui Dirac = constantă \\(\\forall \\omega\\) \\[S_{ff}(\\omega) = constant, \\forall \\omega \\]\n\nputere constantă pentru toate frecvențele, până la \\(f = \\infty\\)\n\nZgomotul alb poate avea orice distribuție (normală, uniformă etc.)\n\ntermenul “zgomot alb” nu se referă la distribuția eșantioanelor, ci la faptul că valorile eșantioanele sunt necorelate\n\n\n\n\n2.6.6 Zgomot alb de bandă limitată\n\nÎn lumea reală, pentru orice semnal puterea scade la 0 la frecvențe foarte înalte\n\npentru că puterea totală \\(P = \\int_{-\\infty}^{\\infty} S_{ff}{\\omega}\\) nu poate fi infinită\nse numește zgomot alb de bandă limitată\n\nÎn acest caz, autocorelația = aproximativ un Dirac, dar nu chiar infinit de “subțire”\n\neșantioane foarte apropiate sunt totuși corelate\nde ex. din cauza unor mici capacități parazite\n\n\n\n\n2.6.7 AWGN\n\nAWGN = Additive White Gaussian Noise\n\nZgomot alb, Gaussian, aditiv\ntipul/modelul de zgomot cel mai frecvent întâlnit în aplicații\n\nÎnseamnă:\n\nzgomot: este un proces aleator (fiecare eșantion este aleator, fiecare realizare este diferită)\ngaussian: eșantioanele au distribuția normală\nalb: valorile eșantioanelor sunt necorelate între ele\naditiv: zgomotul se adună peste semnalul original (adică de ex. nu se multiplică cu acesta)\n\n\n\n\n2.6.8 Examen 2020-2021\n\nPână aici s-a făcut în 2020-2021. Celelalte slide-uri din acest fișier nu se cer.\n\n\n\n2.6.9 Proprietățile funcției de autocorelație\n\nEste o funcție pară \\[R_{ff}(\\tau) = R_{ff}(-\\tau)\\]\n\nDemonstrație: Schimbare de variabilă în definiție\n\nLa infinit, tinde la o valoare constantă \\[R_{ff}(\\infty) = \\overline{f(t)}^2 = const\\]\n\nDem.: două eșantioane la un interval \\(\\infty\\) sunt necesar independente\n\nAre valoarea maximă în 0 \\[R_{ff}(0) \\geq R_{ff}(\\tau)\\]\n\nDem.: se pornește de la \\(\\overline{(f(t) - f(t + \\tau))^2} \\geq 0\\)\nInterpretare: eșantioane diferite mai pot varia diferit, dar un eșantion variază întotdeauna identic cu sine însuși\n\n\n\n\n2.6.10 Proprietățile funcției de autocorelație\n\nValoarea în 0 = puterea procesului aleator \\[R_{ff}(0) = \\frac{1}{2 \\pi} \\int_{-\\infty}^{\\infty} S_{ff}(\\omega) d\\omega\\]\n\nDem.: Se pune \\(\\tau = 0\\) în transf. Fourier inversă din teorema Wiener-Hincin\n\nVarianța = diferența între valoarea din 0 și cea de la \\(\\infty\\) \\[\\sigma^2 = R_{ff}(0) - R_{ff}(\\infty)\\]\n\nDem.: \\(R_{ff}(0) = \\overline{f(t)^2}\\), \\(R_{ff}(\\infty) = \\overline{f(t)}^2\\)\n\n\n\n\n2.6.11 Autocorelația unui proces aleator filtrat\n\nFie un proces aleator aplicat la intrarea unui sistem\n\nfie în timp continuu: intrarea \\(x(t)\\), sistemul \\(H(s)\\), ieșirea \\(y(t)\\)\nfie în timp discret: intrarea \\(x[n]\\), sistemul \\(H(z)\\), ieșirea \\(y[n]\\)\n\nCum depinde autocorelația ieșirii \\(y\\) de cea a intrării \\(x\\)?\nSe știe că \\(y\\) este convoluția lui \\(x\\) cu răspunsul la impuls \\(h\\)\n\n\n\n2.6.12 Dezvoltare matematică\n\nPentru un proces aleator în timp discret \\[\\begin{split}\nR_{yy}(\\tau) =& \\overline{y[n] y[n + \\tau]}\\\\\n=& \\overline{\\sum_{k_1=-\\infty}^\\infty h[k_1] x[n-k_1] \\sum_{k_2=-\\infty}^\\infty h[k_2] x[n+\\tau-k_2]}\\\\\n=& \\sum_{k_1=-\\infty}^\\infty \\sum_{k_2=-\\infty}^\\infty h[k_1] h[k_2] \\overline{x[n-k_1] x[n+\\tau-k_2]}\\\\\n=& \\sum_{k_1=-\\infty}^\\infty \\sum_{k_2=-\\infty}^\\infty h[k_1] h[k_2] R_{xx}[\\tau - k_1 + k_2]\n\\end{split}\\]\nDin teorema Wiener-Hincin se știe că: \\[S_{ff}(\\omega) = \\sum_{\\tau = -\\infty}^{\\infty} R_{ff}(\\tau) e^{- j \\omega \\tau}\\]\n\n\n\n2.6.13 Dezvoltare matematică\n\nAșadar \\[\nS_{yy}(\\omega) = \\sum_{\\tau=-\\infty}^{\\infty} \\sum_{k_1=-\\infty}^\\infty \\sum_{k_2=-\\infty}^\\infty h[k_1] h[k_2] R_{xx}[\\tau - k_1 + k_2] e^{- j \\omega \\tau}\n\\]\nSchimbare de variabilă \\(\\tau - k_1 + k_2 = u\\)\n\nrezultă \\(\\tau = u + k_1 - k_2\\)\n\n\n\\[\\begin{split}\nS_{yy}(\\omega) =& \\sum_{u=-\\infty}^{\\infty} \\sum_{k_1=-\\infty}^\\infty \\sum_{k_2=-\\infty}^\\infty h[k_1] h[k_2] R_{xx}[u] e^{- j \\omega (u + k_1 + k_2)}\\\\\n=& \\sum_{u=-\\infty}^{\\infty} R_{xx}[u] e^{- j \\omega u} \\sum_{k_1=-\\infty}^\\infty h[k_1] e^{- j \\omega k_1} \\sum_{k_2=-\\infty}^\\infty h[k_2]  e^{ j \\omega k_2}\\\\\n=& S_{xx}(\\omega) \\cdot H(\\omega) \\cdot H*^(\\omega)\\\\\n=& S_{xx}(\\omega) \\cdot |H(\\omega)|^2\\\\\n\\end{split}\\]\n\n\n2.6.14 Rezultat\n\\[S_{yy}(\\omega) = S_{xx}(\\omega) \\cdot |H(\\omega)|^2\\]\n\nDSP a lui \\(y\\) = DSP a lui \\(x\\) multiplicată cu răspunsul în amplitudine, la pătrat, al filtrului\nRelația este valabilă și pentru procese aleatoare continue\n\n\n\n2.6.15 Aplicații ale (auto)corelației\n\nCăutarea unei anume porțiuni într-un semnal mai mare\nCorelația a două semnale = o măsura a similarității celor două semnale\n\nFuncția de corelație măsoară similaritatea unui semnal cu toate versiunile decalate ale celuilalt\nExemplu numeric la tablă, semnale de lungime finită\n\nCorelația poate fi utilizată pentru localizare\n\nFuncția de (auto)corelație are valori mari atunci când cele două semnale se potrivesc\nValori mari sunt atunci când valorile pozitive / negative ale semnalelor se potrivesc\nValori mici atunci când nu se potrivesc\n\n\n\n\n2.6.16 Semnalul căutat\nimport matplotlib.pyplot as plt, numpy as np\nx1 = np.array([1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1])\nx2 = np.hstack((np.random.randn(800), x1, np.random.randn(300)))\ncorr = np.correlate(x2, x1)\nplt.figure(figsize=(12,6))\nplt.stem(x1); plt.title ('Signal to look for');plt.axis([0, 20, -1.5, 1.5])\nplt.savefig('fig/01_RandomSignals_CorrSearch_Pattern.png', transparent=True, bbox_inches='tight', dpi=300)\nplt.close()\n\n\n\n2.6.17 Semnalul de dimensiuni mari\nimport matplotlib.pyplot as plt, numpy as np\nx1 = np.array([1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1])\nx2 = np.hstack((np.random.randn(800), x1, np.random.randn(300)))\ncorr = np.correlate(x2, x1)\nplt.figure(figsize=(12,6))\nplt.stem(x2); plt.title ('Signal to search in');\nplt.savefig('fig/01_RandomSignals_CorrSearch_CompleteSignal.png', transparent=True, bbox_inches='tight', dpi=300)\nplt.close()\n\n\n\n2.6.18 Rezultatul corelației\nimport matplotlib.pyplot as plt, numpy as np\nx1 = np.array([1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1])\nx2 = np.hstack((np.random.randn(800), x1, np.random.randn(300)))\ncorr = np.correlate(x2, x1)\nplt.figure(figsize=(12,6))\nplt.stem(corr); plt.title ('Correlation signal');\nplt.savefig('fig/01_RandomSignals_CorrSearch_Result.png', transparent=True, bbox_inches='tight', dpi=300)\nplt.close()\n\n\n\n2.6.19 Identificare de sistem\n\nDeterminarea răspunsului la impuls al unui sistem necunoscut, liniar și invariant în timp\nSe bazează pe corelația intrării cu ieșirea sistemlui\n\n\n\n\nSystem identification setup\n\n\n\n\n2.6.20 Identificare de sistem\n\\[\\begin{split}\nR_{fg}(\\tau) =& \\overline{f[n] g[n + \\tau]}\\\\\n=& \\overline{f[n] \\sum_{k=-\\infty}^\\infty h[k] f[n+\\tau-k]}\\\\\n=& \\sum_{k=-\\infty}^\\infty h[k] \\overline{f[n] f[n+\\tau-k]}\\\\\n=& \\sum_{k=-\\infty}^\\infty h[k] R_{ff}[\\tau - k]\\\\\n=& h[\\tau] \\star R_{ff}[\\tau]\n\\end{split}\\]\n\nDacă intrarea \\(f\\) este zgomot alb cu puterea \\(A\\), \\(R_{ff}[n] = A \\cdot \\delta[n]\\), și \\[R_{fg}(\\tau) = h[\\tau] \\star R_{ff}[\\tau] = A \\cdot h[\\tau] \\star \\delta[\\tau] = A \\cdot h[\\tau]\\]\nCorelația măsurată este proporțională cu răspunsul la impuls al sistemului necunoscut"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  }
]