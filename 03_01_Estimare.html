<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Decizie și estimare în prelucrarea informației (DEPI) - 7&nbsp; Elemente de teoria estimării</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ex_Seminar01.html" rel="next">
<link href="./02_03_AlgoritmiPractici.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Elemente de teoria estimării</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Decizie și estimare în prelucrarea informației (DEPI)</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Prefață</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_Intro.html" class="sidebar-item-text sidebar-link">Introducere</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Semnale Aleatoare</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_01_VariabileAleatoare.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Variabile aleatoare</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_02_ProceseAleatoare.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Procese aleatoare</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_03_Autocorelatia.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Proprietăți ale autocorelației</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Decizie</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_01_Detectia1Esantion.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Decizie cu un singur eșantion</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_02_DetectiaMaiMulteEsantioane.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Decizie cu mai multe eșantioane</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_03_AlgoritmiPractici.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Algoritmi practici</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Estimare</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_01_Estimare.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Elemente de teoria estimării</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Exerciții seminar</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ex_Seminar01.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Seminar 01: Variabile aleatoare și probabilități</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ex_Seminar02.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Seminar 02: Medii statistice și temporale</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ex_Seminar03.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Seminar 03: Criteriul de decizie Maximum Likelihood</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ex_Seminar04.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Seminar 04: Decizii, decizii</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ex_Seminar05.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Seminar 05: Decizii cu mai multe eșantioane</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ex_Seminar06.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Seminar 06: K-NN, K-Means, estimare ML</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">Aplicații practice</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./App01_OutlierDetection.html" class="sidebar-item-text sidebar-link">Outlier Detection with the 3-sigma Rule</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#ce-înseamnă-estimare" id="toc-ce-înseamnă-estimare" class="nav-link active" data-scroll-target="#ce-înseamnă-estimare"><span class="toc-section-number">7.1</span>  Ce înseamnă “estimare”?</a></li>
  <li><a href="#definirea-problemei" id="toc-definirea-problemei" class="nav-link" data-scroll-target="#definirea-problemei"><span class="toc-section-number">7.2</span>  Definirea problemei</a></li>
  <li><a href="#estimarea-de-plauzibilitate-maximă-maximum-likelihood" id="toc-estimarea-de-plauzibilitate-maximă-maximum-likelihood" class="nav-link" data-scroll-target="#estimarea-de-plauzibilitate-maximă-maximum-likelihood"><span class="toc-section-number">7.3</span>  Estimarea de plauzibilitate maximă (Maximum Likelihood)</a>
  <ul class="collapse">
  <li><a href="#tipuri-de-estimare" id="toc-tipuri-de-estimare" class="nav-link" data-scroll-target="#tipuri-de-estimare"><span class="toc-section-number">7.3.1</span>  Tipuri de estimare</a></li>
  <li><a href="#estimarea-tip-maximum-likelihood" id="toc-estimarea-tip-maximum-likelihood" class="nav-link" data-scroll-target="#estimarea-tip-maximum-likelihood"><span class="toc-section-number">7.3.2</span>  Estimarea tip Maximum Likelihood</a></li>
  <li><a href="#rezolvare-matematică" id="toc-rezolvare-matematică" class="nav-link" data-scroll-target="#rezolvare-matematică"><span class="toc-section-number">7.3.3</span>  Rezolvare matematică</a></li>
  <li><a href="#estimarea-ml-în-zgomot-de-tip-awgn" id="toc-estimarea-ml-în-zgomot-de-tip-awgn" class="nav-link" data-scroll-target="#estimarea-ml-în-zgomot-de-tip-awgn"><span class="toc-section-number">7.3.4</span>  Estimarea ML în zgomot de tip AWGN</a></li>
  <li><a href="#parametri-multipli" id="toc-parametri-multipli" class="nav-link" data-scroll-target="#parametri-multipli"><span class="toc-section-number">7.3.5</span>  Parametri multipli</a></li>
  </ul></li>
  <li><a href="#deplasarea-și-varianța-estimatorilor" id="toc-deplasarea-și-varianța-estimatorilor" class="nav-link" data-scroll-target="#deplasarea-și-varianța-estimatorilor"><span class="toc-section-number">7.4</span>  Deplasarea și varianța estimatorilor</a></li>
  <li><a href="#estimare-bayesiană" id="toc-estimare-bayesiană" class="nav-link" data-scroll-target="#estimare-bayesiană"><span class="toc-section-number">7.5</span>  Estimare Bayesiană</a>
  <ul class="collapse">
  <li><a href="#distribuția-a-posteriori" id="toc-distribuția-a-posteriori" class="nav-link" data-scroll-target="#distribuția-a-posteriori"><span class="toc-section-number">7.5.1</span>  Distribuția <em>a posteriori</em></a></li>
  <li><a href="#estimatorul-map" id="toc-estimatorul-map" class="nav-link" data-scroll-target="#estimatorul-map"><span class="toc-section-number">7.5.2</span>  Estimatorul MAP</a></li>
  <li><a href="#funcții-de-cost" id="toc-funcții-de-cost" class="nav-link" data-scroll-target="#funcții-de-cost"><span class="toc-section-number">7.5.3</span>  Funcții de cost</a></li>
  <li><a href="#estimatorul-epmm" id="toc-estimatorul-epmm" class="nav-link" data-scroll-target="#estimatorul-epmm"><span class="toc-section-number">7.5.4</span>  Estimatorul EPMM</a></li>
  <li><a href="#estimatorul-map-1" id="toc-estimatorul-map-1" class="nav-link" data-scroll-target="#estimatorul-map-1"><span class="toc-section-number">7.5.5</span>  Estimatorul MAP</a></li>
  <li><a href="#estimatorul-map-2" id="toc-estimatorul-map-2" class="nav-link" data-scroll-target="#estimatorul-map-2"><span class="toc-section-number">7.5.6</span>  Estimatorul MAP</a></li>
  <li><a href="#interpretare" id="toc-interpretare" class="nav-link" data-scroll-target="#interpretare"><span class="toc-section-number">7.5.7</span>  Interpretare</a></li>
  <li><a href="#relația-între-estim.-map-and-epmm" id="toc-relația-între-estim.-map-and-epmm" class="nav-link" data-scroll-target="#relația-între-estim.-map-and-epmm"><span class="toc-section-number">7.5.8</span>  Relația între estim. MAP and EPMM</a></li>
  <li><a href="#exercițiu" id="toc-exercițiu" class="nav-link" data-scroll-target="#exercițiu"><span class="toc-section-number">7.5.9</span>  Exercițiu</a></li>
  <li><a href="#exercițiu-1" id="toc-exercițiu-1" class="nav-link" data-scroll-target="#exercițiu-1"><span class="toc-section-number">7.5.10</span>  Exercițiu</a></li>
  <li><a href="#semnal-oarecare-în-zgomot-gaussian-awgn" id="toc-semnal-oarecare-în-zgomot-gaussian-awgn" class="nav-link" data-scroll-target="#semnal-oarecare-în-zgomot-gaussian-awgn"><span class="toc-section-number">7.5.11</span>  Semnal oarecare în zgomot Gaussian (AWGN)</a></li>
  <li><a href="#semnal-oarecare-în-zgomot-gaussian-awgn-1" id="toc-semnal-oarecare-în-zgomot-gaussian-awgn-1" class="nav-link" data-scroll-target="#semnal-oarecare-în-zgomot-gaussian-awgn-1"><span class="toc-section-number">7.5.12</span>  Semnal oarecare în zgomot Gaussian (AWGN)</a></li>
  <li><a href="#distribuție-a-priori-gaussiană" id="toc-distribuție-a-priori-gaussiană" class="nav-link" data-scroll-target="#distribuție-a-priori-gaussiană"><span class="toc-section-number">7.5.13</span>  Distribuție “a priori” Gaussiană</a></li>
  <li><a href="#interpretare-1" id="toc-interpretare-1" class="nav-link" data-scroll-target="#interpretare-1"><span class="toc-section-number">7.5.14</span>  Interpretare</a></li>
  <li><a href="#aplicații" id="toc-aplicații" class="nav-link" data-scroll-target="#aplicații"><span class="toc-section-number">7.5.15</span>  Aplicații</a></li>
  <li><a href="#aplicații-practice" id="toc-aplicații-practice" class="nav-link" data-scroll-target="#aplicații-practice"><span class="toc-section-number">7.5.16</span>  Aplicații practice</a></li>
  <li><a href="#single-object-tracking" id="toc-single-object-tracking" class="nav-link" data-scroll-target="#single-object-tracking"><span class="toc-section-number">7.5.17</span>  Single object tracking</a></li>
  <li><a href="#single-object-tracking-1" id="toc-single-object-tracking-1" class="nav-link" data-scroll-target="#single-object-tracking-1"><span class="toc-section-number">7.5.18</span>  Single object tracking</a></li>
  <li><a href="#aplicații-practice-1" id="toc-aplicații-practice-1" class="nav-link" data-scroll-target="#aplicații-practice-1"><span class="toc-section-number">7.5.19</span>  Aplicații practice</a></li>
  <li><a href="#constrained-least-squares-cls-image-restoration" id="toc-constrained-least-squares-cls-image-restoration" class="nav-link" data-scroll-target="#constrained-least-squares-cls-image-restoration"><span class="toc-section-number">7.5.20</span>  Constrained Least Squares (CLS) image restoration</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Elemente de teoria estimării</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="hidden">
<p><span class="math display">\[
\newcommand{\grtlessH}{\underset{{H_0}}{\overset{H_{1}}{\gtrless}}}
\renewcommand{\vec}[1]{\mathbf{#1}}
\]</span></p>
</div>
<section id="ce-înseamnă-estimare" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="ce-înseamnă-estimare"><span class="header-section-number">7.1</span> Ce înseamnă “estimare”?</h2>
<p>Vom considera următoarea problemă de bază.</p>
<p>Un emițător transmite un semnal <span class="math inline">\(s_\Theta(t)\)</span>, care depinde de parametru <strong>necunoscut</strong> <span class="math inline">\(\Theta\)</span> (de ex. amplitudinea e necunoscută). În afara acestui parametru, semnalul este cunoscut în totalitate.</p>
<p>Semnalul acesta ajunge la receptor afectat de zgomotul <span class="math inline">\(n(t)\)</span>, astfel încât semnalul recepționat <span class="math inline">\(r(t)\)</span> este: <span class="math display">\[r(t) = s_\Theta(t) + n(t) \]</span></p>
<p>Teoria estimării se ocupă de problema estimării parametrului <span class="math inline">\(\Theta\)</span>, cunoscând semnalul recepționat și parametrii statistici ai zgomotului.</p>
<p>Întrucât nu cunoaștem valorile exacte ale zgomotului <span class="math inline">\(n(t)\)</span>, nu putem găsi valoarea adevărată a parametrului necunoscut <span class="math inline">\(\Theta\)</span>, tot ceea ce putem face este să o <strong>estimăm</strong>. Valoarea găsită în urma estimării va fi notată <span class="math inline">\(\hat{\Theta}\)</span>, și se numește <strong>estimatul</strong> lui <span class="math inline">\(\Theta\)</span></p>
<p>Există întotdeauna o eroare de estimare <span class="math inline">\(\epsilon\)</span> între valoarea adevărată a lui <span class="math inline">\(\Theta\)</span> și estimatul său de la recepție: <span class="math display">\[\epsilon = \hat{\Theta} - \Theta\]</span></p>
<p>Exemple:</p>
<ul>
<li><p>Amplitudinea unui semnal constant: <span class="math inline">\(r(t) = A + zgomot\)</span>, trebuie estimat <span class="math inline">\(A\)</span></p></li>
<li><p>Faza unui semnal sinusoidal: <span class="math inline">\(r(t) = \cos(2 \pi f t + \phi) + zgomot\)</span>, de estimat <span class="math inline">\(\phi\)</span></p></li>
<li><p>Exemple mai complicate:</p>
<ul>
<li>De estimat/decis ce cuvânt este pronunțat într-un semnal vocal</li>
</ul></li>
<li><p>Fie următoarea problemă de estimare:</p>
<p>Se recepționează un semnal <span class="math inline">\(r(t) = A + zgomot\)</span>, estimați-l pe <span class="math inline">\(A\)</span></p></li>
<li><p>La detecție: se alege între <strong>două valori cunoscute</strong> ale <span class="math inline">\(A\)</span>:</p>
<ul>
<li>de ex. <span class="math inline">\(A\)</span> poate fi 0 sau 5 (ipotezele <span class="math inline">\(H_0\)</span> și <span class="math inline">\(H_1\)</span>)</li>
</ul></li>
<li><p>La estimare: <span class="math inline">\(A\)</span> poate fi oricât =&gt; se alege între <strong>o infinitate de opțiuni</strong> ale <span class="math inline">\(A\)</span></p>
<ul>
<li><span class="math inline">\(A\)</span> poate fi orice valoare din <span class="math inline">\(\mathbb{R}\)</span>, în general</li>
</ul></li>
</ul>
<p>Există o strânsă legătură între problemele de decizie și cele de estimare. Practic, prin “decizie” înțelegem o problemă de estimare <strong>restrânsă</strong> doar la un set discret, redus, de opțiuni; se alege doar una dintre cele câteva ipoteze. La rândul său, o problemă de estimare poate fi văzută ca o problemă de detecție, dar cu un număr <strong>infinit de opțiuni</strong> posibile. Metodele statistice folosite sunt foarte similare în ambele situații</p>
</section>
<section id="definirea-problemei" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="definirea-problemei"><span class="header-section-number">7.2</span> Definirea problemei</h2>
<p>Considerăm un semnal recepționat <span class="math inline">\(r(t)\)</span>, depinzând de parametrul cu valoare necunoscută <span class="math inline">\(\Theta\)</span>, și afectat de zgomot aditiv:</p>
<p><span class="math display">\[r(t) = s_\Theta(t) + zgomot\]</span></p>
<p>Din <span class="math inline">\(r(t)\)</span> se iau <strong>N eșantioane</strong>, la momentele de timp <span class="math inline">\(t_i\)</span>,</p>
<p><span class="math display">\[r_i = r(t_i) = s_\Theta(t_i) + n(t_i)\]</span></p>
<p>obținându-se vectorul de eșantioane:</p>
<p><span class="math display">\[\vec{r} = [r_1, r_2, ... r_N]\]</span></p>
<p>Fiecare eșantion <span class="math inline">\(r_i = s_\Theta(t_i) + n(t_i)\)</span> este o variabilă aleatoare, având aceeași distribuție ca eșantionul de zgomot <span class="math inline">\(n_(t_i)\)</span>, dar translată cu <span class="math inline">\(s_\Theta(t_i)\)</span>. Distribuția eșantionului <span class="math inline">\(r_i\)</span>, care depinde așadar de <span class="math inline">\(\Theta\)</span>, este notată:</p>
<p><span class="math display">\[w_i(r_i | \Theta)\]</span></p>
<p>Întregul vector de eșantioane <span class="math inline">\(\vec{r}\)</span> este o variabilă aleatoare N-dimensională ce depinde de <span class="math inline">\(\Theta\)</span>, și are o distribuție N-dimensională <span class="math inline">\(w(\vec{r} | \Theta)\)</span>. Dacă zgomotul este alb, autnci eșantioane diferite de zgomot sunt independente, iar distribuția vectorului <span class="math inline">\(\vec{r}\)</span> devine egală cu produsul distribuțiilor fiecărui eșantion:</p>
<p><span class="math display">\[w(\vec{r} | \Theta) = w_1(r_1 | \Theta) \cdot w_2(r_2 | \Theta) \cdot ... \cdot w_N(r_N | \Theta)\]</span></p>
</section>
<section id="estimarea-de-plauzibilitate-maximă-maximum-likelihood" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="estimarea-de-plauzibilitate-maximă-maximum-likelihood"><span class="header-section-number">7.3</span> Estimarea de plauzibilitate maximă (Maximum Likelihood)</h2>
<section id="tipuri-de-estimare" class="level3" data-number="7.3.1">
<h3 data-number="7.3.1" class="anchored" data-anchor-id="tipuri-de-estimare"><span class="header-section-number">7.3.1</span> Tipuri de estimare</h3>
<p>Există două abordări ale problemelor de estimare:</p>
<ol type="1">
<li><p><strong>Estimare de plauzibilitate maximă</strong> (Maximum Likelihood Estimation, MLE): În afară de <span class="math inline">\(\vec{r}\)</span> nu se cunoaște nimic despre <span class="math inline">\(\Theta\)</span>, decât cel mult vreun domeniu de existență (de ex. <span class="math inline">\(\Theta &gt; 0\)</span>)</p>
<p>Estimarea ML este un caz particular al celei de-a doua abordări.</p></li>
<li><p><strong>Estimare Bayesiană</strong>: În afară de <span class="math inline">\(\vec{r}\)</span> se mai cunoaște o distribuție <em>a priori</em> <span class="math inline">\(w(\Theta)\)</span> a lui <span class="math inline">\(\Theta\)</span>, care indică ce valori ale lui <span class="math inline">\(\Theta\)</span> sunt mai probabile sau mai puțin probabile.</p></li>
</ol>
</section>
<section id="estimarea-tip-maximum-likelihood" class="level3" data-number="7.3.2">
<h3 data-number="7.3.2" class="anchored" data-anchor-id="estimarea-tip-maximum-likelihood"><span class="header-section-number">7.3.2</span> Estimarea tip Maximum Likelihood</h3>
<p>Dacă nu se cunoaște vreo distribuție <em>a priori</em> se folosește metoda estimării de plauzibilitate maximă (“Maximum Likelihood”, ML)</p>
<p>Dat fiind vectorul de observații <span class="math inline">\(\vec{r}\)</span> cunoscut, se definește <strong>plauzibilitatea</strong> unui valori <span class="math inline">\(\Theta\)</span> astfel:</p>
<p><span class="math display">\[L(\Theta | \vec{r}) = w(\Theta | \vec{r})\]</span></p>
<p>Funcția <span class="math inline">\(L(\Theta | \vec{r})\)</span> reprezintă funcția de plauzibilitate, care asociază pentru fiecare valoare posibilă <span class="math inline">\(\Theta\)</span> a parametrului căutat o anumită plauzibilitate. Conform ecuației, plauzibilitatea unei valori oarecare <span class="math inline">\(\Theta\)</span> reprezintă “probabilitatea” ca, dacă parametrul avea acea valoare, să se obțină vectorul $ care s-a obținut.</p>
<p>Comparând cu formula din Cap. 2, slide 20, se observă ce e aceeași definiție, cu diferența că aici estimăm pe <span class="math inline">\(\Theta\)</span>, iar în acel caz “estimam” ipoteza corectă <span class="math inline">\(H_i\)</span>.</p>
<p>Estimarea de plauzibilitate maximă (Maximum Likelihood, ML) constă în alege estimatul <span class="math inline">\(\hat{\Theta}_{ML}\)</span> ca fiind <strong>valoarea care maximizează plauzibilitatea, dat fiind valorile observate <span class="math inline">\(\vec{r}\)</span></strong>, adică valoarea care maximizează <span class="math inline">\(L(\Theta | \vec{r})\)</span>:</p>
<p><span class="math display">\[\hat{\Theta}_{ML} = \arg\max_{\Theta} L(\Theta | \vec{r}) = \arg\max_{\Theta} w(\vec{r} | \Theta)\]</span></p>
<p>În general, se caută valoarea care maximizează funcția pe întreaga axă <span class="math inline">\(\mathbb{R}\)</span>. Dacă se cunoaște că <span class="math inline">\(\Theta\)</span> aparține doar unui anumit interval, se poate face maximizarea doar pe acel interval.</p>
<p>Reamintim, in acest context, notațiile matematice generale</p>
<ul>
<li><span class="math inline">\(\arg\max_{x} f(x)\)</span> este “valoarea <span class="math inline">\(x\)</span> are maximizează funcția f(x)”</li>
<li><span class="math inline">\(\max_{x} f(x)\)</span> = “valoarea maximă a funcției f(x)”</li>
</ul>
<p>Se observă faptul că estimarea ML este foarte similară cu decizia ML. Criteriul de decizie ML înseamnă, de fapt, alegerea ipotezei cu plauzibilitate mai mare: <span class="math display">\[\frac{L(H_1 | r)}{L(H_0 | r)} = \frac{w(r|H_1)}{w(r|H_0)} \grtlessH 1\]</span> Același lucru este valabil și la estimare, cu diferența că nu se alege între două alternative, ci se alege valoarea reală din întregul <span class="math inline">\(\mathbb{R}\)</span> care maximizează plauzibilitatea: <span class="math display">\[\hat{\Theta}_{ML} = \arg\max_{\Theta } L(\Theta | \vec{r}) = \arg\max_{\Theta} w(\vec{r} | \Theta)\]</span></p>
</section>
<section id="rezolvare-matematică" class="level3" data-number="7.3.3">
<h3 data-number="7.3.3" class="anchored" data-anchor-id="rezolvare-matematică"><span class="header-section-number">7.3.3</span> Rezolvare matematică</h3>
<p>În continuare, ne punem problema cum se rezolvă problema de maximizare, adică cum se găsește estimatul <span class="math inline">\(\hat{\Theta}_{ML}\)</span> care maximizează <span class="math inline">\(L(\Theta | vec{r})\)</span>.</p>
<p>Ca la orice funcție continuă, maximul se găsește prin derivare și egalare cu 0: <span class="math display">\[\frac{d L(\Theta | \vec{r})}{d\Theta} = 0\]</span> În plus, se poate aplica logaritmul natural asupra funcției <span class="math inline">\(L(\Theta | \vec{r})\)</span> înainte de derivare, astfel încât se derivează <span class="math inline">\(\ln{L(\cdot)}\)</span> (funcția “log-likelihood”): <span class="math display">\[\frac{d \ln\left(L(\Theta | \vec{r})\right)}{d\Theta} = 0\]</span> Acest lucru ajută la simplificarea expresiilor de tip exponențial, și nu schimbă valoarea în dreptul căreia se găsește maximul funcției.</p>
<p>Procedura de găsire a estimatului ML este detaliată mai jos.</p>
<ol type="1">
<li>Se găsește expresia funcției <span class="math display">\[L(\Theta | \vec{r}) = w(\vec{r} | \Theta)\]</span></li>
<li>Se pune condiția ca derivata lui <span class="math inline">\(L(\Theta | \vec{r})\)</span> sau a lui <span class="math inline">\(\ln(\left(L(\Theta | \vec{r})\right)\)</span> să fie 0 <span class="math display">\[\frac{d L(\Theta)}{d\Theta} = 0, \text{ sau }\frac{d \ln\left(L(\Theta)\right)}{d\Theta} = 0\]</span></li>
<li>Se rezolvă ecuația, se găsește valoarea <span class="math inline">\(\hat{\Theta}_{ML}\)</span></li>
<li>Se verifică că derivata a doua în punctul <span class="math inline">\(\hat{\Theta}_{ML}\)</span> este negativă, pentru a verifica că este un punct de maxim (și nu de minim). Reamintim că derivata este nulă și pentru maxime și pentru minime. Uneori putem ignora această etapă, dacă se cunoaște faptul că funcția are doar un punct de extrem.</li>
</ol>
<div class="callout-tip callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Exemplu: semnal constant în zgomot gaussian
</div>
</div>
<div class="callout-body-container callout-body" title="Exemplu">
<p>Găsiți estimatul Maximum Likelihood pentru un semnal de valoare constantă <span class="math inline">\(s_\Theta(t) = A\)</span> din 5 măsurători afectate de zgomot <span class="math inline">\(r_i = A + zgomot\)</span>, cu valori egale cu <span class="math inline">\([5, 7, 8, 6.1, 5.3]\)</span>. Zgomotul este AWGN <span class="math inline">\(\mathcal{N}(\mu=0, \sigma^2)\)</span>.</p>
<p>Soluție: la tablă</p>
<p>Estimatul <span class="math inline">\(\hat{A}_{ML}\)</span> este chiar valoarea medie a eșantioanelor (deloc surprinzător).</p>
<div class="sourceCode" id="cb1" data-session="plot"><pre class="sourceCode python cb.run code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt, numpy <span class="im">as</span> np, math<span class="op">;</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> np.array([<span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="fl">6.1</span>, <span class="fl">5.3</span>])</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>plt.stem(r, basefmt<span class="op">=</span><span class="st">" "</span>, use_line_collection<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>plt.plot(np.mean(r) <span class="op">*</span> np.ones(r.shape), color <span class="op">=</span> <span class="st">'red'</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Esantioane'</span>)<span class="op">;</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Valori'</span>)<span class="op">;</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Estimarea unui semnal constant'</span>)<span class="op">;</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>plt.legend((<span class="st">'Estimatul'</span>, <span class="st">'Esantioane'</span>))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">'fig/03_NumericalSim_Constant.png'</span>, transparent<span class="op">=</span><span class="va">True</span>, bbox_inches<span class="op">=</span><span class="st">'tight'</span>, dpi<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="fig/03_NumericalSim_Constant.png" class="img-fluid" style="width:70.0%" data-max-width="1000px"></p>
</div>
</div>
<p>O problemă de estimare poate fi înțeleasă ca o problemă de aproximare a unei curbe, găsind cea mai bună potrivire a lui <span class="math inline">\(s_\Theta(t)\)</span> prin datele <span class="math inline">\(\vec{r}\)</span>. Elementele cheie se observă din exemplul grafic anterior:</p>
<ul>
<li>avem un set de date <span class="math inline">\(\vec{r}\)</span>, care reprezintă o serie de puncte măsurate</li>
<li>se cunoaște forma semnalului, adică o dreaptă orizontală (<span class="math inline">\(A\)</span> constant)</li>
<li>prin estimare se aproximează în mod optim poziția dreaptei prin setul de date</li>
</ul>
</section>
<section id="estimarea-ml-în-zgomot-de-tip-awgn" class="level3" data-number="7.3.4">
<h3 data-number="7.3.4" class="anchored" data-anchor-id="estimarea-ml-în-zgomot-de-tip-awgn"><span class="header-section-number">7.3.4</span> Estimarea ML în zgomot de tip AWGN</h3>
<p>Fie semnalul original <span class="math inline">\(s_\Theta(t)\)</span>, și zgomotul de tip AWGN <span class="math inline">\(\mathcal{N}(\mu=0, \sigma^2)\)</span>.</p>
<p>Eșantioanele <span class="math inline">\(r_i\)</span> sunt luate la momentele <span class="math inline">\(t_i\)</span>, și vor avea distribuție normală, cu media <span class="math inline">\(\mu = s_\Theta(t_i)\)</span> și varianța <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Funcția de plauzibilitate <span class="math inline">\(L(\Theta | \vec{r})\)</span> a întregului vector <span class="math inline">\(\vec{r}\)</span> se poate descompune ca produsul plauzibilităților fiecărui eșantion <span class="math inline">\(r_i\)</span> în parte: <span class="math display">\[\begin{split}
L(\Theta | \vec{r}) = w(\vec{r} | \Theta) =&amp; \prod_{i=1}^N \frac{1}{\sigma \sqrt{2 \pi}} e^{- \frac{(r_i - s_\Theta(t_i))^2}{2 \sigma^2}} \\
=&amp;  \left( \frac{1}{\sigma \sqrt{2 \pi}} \right)^N e^{- \frac{\sum(r_i - s_\Theta(t_i))^2}{2 \sigma^2}}
\end{split}\]</span></p>
<p>Logaritmul plauzibilității (“log-likelihood”) este: <span class="math display">\[\begin{split}
\ln\left(L(\Theta | \vec{r})\right) =&amp; \underbrace{\ln\left(\frac{1}{\sigma \sqrt{2 \pi}}\right)}_{constant} - \frac{\sum(r_i - s_\Theta(t_i))^2}{2 \sigma^2}
\end{split}\]</span></p>
<p>Maximul funcției se atinge atunci când exponentul este minim: <span class="math display">\[\hat{\Theta}_{ML} = \arg\max_{\Theta} L(\Theta | \vec{r}) = \arg\min \sum(r_i - s_\Theta(t_i))^2\]</span></p>
<p>Se observă că termenul <span class="math inline">\(\sum(r_i - s_\Theta(t_i))^2\)</span> reprezintă chiar distanța Euclideană ridicată la pătrat dintre vectorii cu valorile <span class="math inline">\(\r_i\)</span> și <span class="math inline">\(s_\Theta(t_i)\)</span>: <span class="math display">\[d(\vec{r},s_\Theta) = \sqrt{\sum (r_i - s_\Theta(t_i))^2}\]</span> <span class="math display">\[\left(d(\vec{r},s_\Theta)\right)^2 = \sum (r_i - s_\Theta(t_i))^2\]</span></p>
<p>Așadar, estimarea ML în cazul zgomotului AWGN se poate rescrie sub forma: <span class="math display">\[\hat{\Theta}_{ML} = \arg\max_{\Theta} L(\Theta | \vec{r}) = \arg\min_\Theta d(\vec{r}, \vec{s}_\Theta)^2\]</span></p>
<p>Estimatul de plauzibilitate maximă (ML) <span class="math inline">\(\hat{\Theta}_{ML}\)</span> este acea valoare care face <span class="math inline">\(s_\Theta(t_i)\)</span> <strong>cel mai apropiat de vectorul recepționat <span class="math inline">\(\vec{r}\)</span></strong>. Ca interpretare, o distanță mică înseamnă o potrivire mai bună, deci o plauzibilitate mai bună.</p>
<p>Subliniem că aceeași interpretare bazată pe distanța minimă era valabilă și pentru decizia ML, cu diferența că la decizie se alege minimul dintre două opțiuni, în timp ce aici alegem minimul dintre toate opțiunile posibile.</p>
<p>Relația care definește estimarea ML pentru zgomot AWGN este valabilă pentru orice fel de spații vectoriale (vectori cu N elemente, semnale continue etc). În funcție de natura semnalelor se modifică doar definiția distanței Euclidiene, nu și regula de estimare.</p>
<p>Sumarizând, procedura pentru estimarea tip ML în zgomot AWGN este următoarea.</p>
<ol type="1">
<li><p>Se scrie expresia pentru pătratul distanței: <span class="math display">\[D = \left(d(\vec{r},s_\Theta)\right)^2 = \sum (r_i - s_\Theta(t_i))^2\]</span></p></li>
<li><p>Se caută minimul expresiei, deci egalăm derivata cu 0: <span class="math display">\[\frac{d D}{d\Theta} = \sum 2 (r_i - s_\Theta(t_i)) (- \frac{d s_\Theta(t_i)}{d\Theta}) = 0\]</span></p></li>
<li><p>Se rezolvă și obținem valoarea <span class="math inline">\(\hat{\Theta}_{ML}\)</span></p></li>
<li><p>Se verifică că derivata a doua în punctul <span class="math inline">\(\hat{\Theta}_{ML}\)</span> este pozitivă, pentru a se verifica că punctul este un minim. Uneori se poate sări peste această etapă, dacă se cunoaște ca funcția are un singur punct de extrem.</p></li>
</ol>
<div class="callout-tip callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Exemplu: estimarea frecvenței a unui semnal sinusoidal
</div>
</div>
<div class="callout-body-container callout-body" title="Exemplu">
<p>Găsiți estimatul Maximum Likelihood pentru frecvența <span class="math inline">\(f\)</span> a unui semnal <span class="math inline">\(s_\Theta(t) = cos(2\pi f t_i)\)</span>, din 10 măsurători afectate de zgomot <span class="math inline">\(r_i = cos(2\pi f t_i) + zgomot\)</span> de valori <span class="math inline">\([...]\)</span>. Zgomotul este AWGN <span class="math inline">\(\mathcal{N}(\mu=0, \sigma^2)\)</span>. Momentele de eșantionare sunt <span class="math inline">\(t_i = [0,1,2,3,4,5,6,7,8,9]\)</span></p>
<ul>
<li>Soluție: la tablă</li>
</ul>
<p>Funcția de plauzibilitate este reprezentată mai jos.</p>
<div class="sourceCode" id="cb2" data-session="plot"><pre class="sourceCode python cb.run code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt, numpy <span class="im">as</span> np, math<span class="op">;</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">102</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="fl">0.2</span><span class="op">;</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>ftrue <span class="op">=</span> <span class="fl">0.07</span><span class="op">;</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> np.arange(<span class="dv">0</span>,<span class="dv">20</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> np.cos(<span class="dv">2</span> <span class="op">*</span> math.pi <span class="op">*</span> ftrue <span class="op">*</span> n) <span class="op">+</span> sigma<span class="op">*</span>np.random.randn(<span class="dv">20</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Log-likelihood function</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>fvalues <span class="op">=</span> np.linspace(<span class="fl">0.04</span>, <span class="fl">0.1</span>, <span class="dv">100</span>)<span class="op">;</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> np.zeros((<span class="dv">1</span>,<span class="dv">500</span>))</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> [np.log(<span class="fl">1.</span><span class="op">/</span>(sigma<span class="op">*</span>math.sqrt(<span class="dv">2</span><span class="op">*</span>math.pi))) <span class="op">-</span> (<span class="bu">sum</span>(r <span class="op">-</span> np.cos(<span class="dv">2</span> <span class="op">*</span> math.pi <span class="op">*</span> fvalue <span class="op">*</span> n))<span class="op">**</span><span class="dv">2</span>)<span class="op">/</span><span class="dv">2</span><span class="op">*</span>sigma<span class="op">*</span>sigma <span class="cf">for</span> fvalue <span class="kw">in</span> fvalues]</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>fhat <span class="op">=</span> np.amax(L)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>plt.plot(fvalues,L)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">'fig/03_NumericalSim_CosineFreq_LogLik.png'</span>, transparent<span class="op">=</span><span class="va">True</span>, bbox_inches<span class="op">=</span><span class="st">'tight'</span>, dpi<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="fig/03_NumericalSim_CosineFreq_LogLik.png" class="img-fluid" style="width:70.0%" data-max-width="1000px"></p>
<div class="sourceCode" id="cb3" data-session="plot"><pre class="sourceCode python cb.run code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt, numpy <span class="im">as</span> np, math<span class="op">;</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">102</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="fl">0.2</span><span class="op">;</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>ftrue <span class="op">=</span> <span class="fl">0.07</span><span class="op">;</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> np.arange(<span class="dv">0</span>,<span class="dv">20</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> np.cos(<span class="dv">2</span> <span class="op">*</span> math.pi <span class="op">*</span> ftrue <span class="op">*</span> n) <span class="op">+</span> sigma<span class="op">*</span>np.random.randn(<span class="dv">20</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Log-likelihood function</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>fvalues <span class="op">=</span> np.linspace(<span class="fl">0.04</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">10</span>, <span class="dv">100</span>)<span class="op">;</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> np.zeros((<span class="dv">1</span>,<span class="dv">500</span>))</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> [np.log(<span class="fl">1.</span><span class="op">/</span>(sigma<span class="op">*</span>math.sqrt(<span class="dv">2</span><span class="op">*</span>math.pi))) <span class="op">-</span> (<span class="bu">sum</span>(r <span class="op">-</span> np.cos(<span class="dv">2</span> <span class="op">*</span> math.pi <span class="op">*</span> fvalue <span class="op">*</span> n))<span class="op">**</span><span class="dv">2</span>)<span class="op">/</span><span class="dv">2</span><span class="op">*</span>sigma<span class="op">*</span>sigma <span class="cf">for</span> fvalue <span class="kw">in</span> fvalues]</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>imax <span class="op">=</span> np.argmax(L)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>fhat <span class="op">=</span> fvalues[imax]</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>plt.stem(n,r, basefmt<span class="op">=</span><span class="st">"b"</span>, use_line_collection<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>ntoplot <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">20</span>,<span class="dv">200</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>plt.plot(ntoplot, np.cos(<span class="dv">2</span> <span class="op">*</span> math.pi <span class="op">*</span> fhat <span class="op">*</span> ntoplot), color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>plt.legend((<span class="st">'Cosinusul estimat'</span>,<span class="st">'Esantioane'</span>))</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Frecventa originala = </span><span class="sc">%f</span><span class="st">, estimatul = </span><span class="sc">%f</span><span class="st">'</span><span class="op">%</span>(ftrue, fhat))</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">'fig/03_NumericalSim_CosineFreq.png'</span>, transparent<span class="op">=</span><span class="va">True</span>, bbox_inches<span class="op">=</span><span class="st">'tight'</span>, dpi<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="fig/03_NumericalSim_CosineFreq.png" class="img-fluid" style="width:70.0%" data-max-width="1000px"></p>
</div>
</div>
</section>
<section id="parametri-multipli" class="level3" data-number="7.3.5">
<h3 data-number="7.3.5" class="anchored" data-anchor-id="parametri-multipli"><span class="header-section-number">7.3.5</span> Parametri multipli</h3>
<p>Ce se întâmplă dacă semnalul depinde de mai mulți parametri, nu doar de unul? De exemplu amplitudinea, frecvența și faza inițială ale unui semnal de tip cosinus: <span class="math display">\[s_\vec{\Theta}(t) = A \cos(2 \pi f t + \phi)\]</span></p>
<p>În acest caz, se va considera <span class="math inline">\(\Theta\)</span> ca fiind un vector care cuprinde toți parametrii necunoscuți: <span class="math display">\[\bm{\Theta} = [\Theta_1, \Theta_2, ... \Theta_M]\]</span> De exemplu, <span class="math inline">\(\bm{\Theta} = [\Theta_1, \Theta_2, \Theta_3] =[A, f, \phi]\)</span>.</p>
<p>Estimarea se poate face cu aceeași procedură, dar în loc de o singură derivată vom avea <span class="math inline">\(M\)</span> derivate, adică se rezolvă sistemul: <span class="math display">\[\begin{cases}
\frac{\partial L}{\partial \Theta_1} = 0 \\
\frac{\partial L}{\partial \Theta_2} = 0 \\
\dots \\
\frac{\partial L}{\partial \Theta_M} = 0 \\
\end{cases}\]</span></p>
<p>În cazuri complicate din aplicații reale, unde pot fi foarte mulți parametri, rezolvarea unui astfel de sistem este dificilă, dacă nu chiar imposibilă. În aceste situații, unde nu se pot găsi valorile optime prin formule directe, se pot folosi algoritmi iterativi tip <strong>coborâre după gradient</strong> (Gradient Descent) care îmbunătățesc valorile în mod progresiv.</p>
<p>Pașii fundamentali ai algoritmilor de tip coborâre după gradient (Gradient Descent) sunt următorii.</p>
<ol type="1">
<li><p>Se inițializează parametrii cu valori aleatoare <span class="math inline">\(\bm{\Theta}^{(0)}\)</span></p></li>
<li><p>Repetă la fiecare iterație <span class="math inline">\(k\)</span>:</p>
<ol type="1">
<li><p>Se calculează funcția <span class="math inline">\(L(\bm{\Theta}^{(k)} | \vec{r})\)</span></p></li>
<li><p>Se calculează derivatele <span class="math inline">\(\frac{\partial L}{\partial \Theta_i^{(k)}}\)</span> pentru toți <span class="math inline">\(\Theta_i\)</span> (“<strong>Gradient</strong>”)</p></li>
<li><p>Se actualizează toate valorile <span class="math inline">\(\Theta_i\)</span> prin scăderea derivatei (“<strong>Descent</strong>”): <span class="math display">\[\Theta_i^{(k+1)} = \Theta_i^{(k)} - \mu \frac{\partial L}{\partial \Theta_i^{(k)}}\]</span></p>
<ul>
<li>sau, sub formă vectorială: <span class="math display">\[\bm{\Theta}^{(k+1)} = \bm{\Theta}^{k} - \mu \frac{\partial L}{\partial \bm{\Theta}^{(k)}}\]</span></li>
</ul></li>
</ol></li>
<li><p>Până la îndeplinirea unui criteriu de terminare (de ex. parametrii nu se mai modifică mult)</p></li>
</ol>
<p>Cel mai proeminent exemplu de utilizare a acestor algoritmi este cel al <strong>Rețele Neurale Artificiale</strong> (a.k.a. “Rețele Neurale”, “Deep Learning”, etc.) care reprezintă motorul tehnicilor moderne de inteligență artificială.</p>
</section>
</section>
<section id="deplasarea-și-varianța-estimatorilor" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="deplasarea-și-varianța-estimatorilor"><span class="header-section-number">7.4</span> Deplasarea și varianța estimatorilor</h2>
<p>Cum caracterizăm calitatea unui estimator?</p>
<p>Un estimator <span class="math inline">\(\hat{\Theta}\)</span> este o variabilă aleatoare, în sensul că poate avea diverse valori, întrucât se calculează pe baza eșantioanelor recepționate, care depind de zgomot. Practic, dacă se repetă aceeași estimare pornind de la eșantioane diferite, vor rezulta mereu valori estimate ușor diferite.</p>
<p>Fiind o variabilă aleatoare, se pot defini următoarele mărimi:</p>
<ul>
<li>valoarea medie a estimatorului: <span class="math inline">\(E \left\{ \hat{\Theta} \right\}\)</span></li>
<li>varianța estimatorului: <span class="math inline">\(E \left\{ (\hat{\Theta} - \Theta)^2 \right\}\)</span></li>
</ul>
<p><strong>Deplasarea</strong> (“bias”) unui estimator se definește ca diferența dintre valoarea medie a estimatorului și valoarea adevărată <span class="math inline">\(\Theta\)</span>: <span class="math display">\[Deplasare = E \left\{ \hat{\Theta} \right\} - \Theta\]</span></p>
<p>Un estimator este <strong>nedeplasat</strong> atunci când valoarea medie a estimatorului este egală cu valoarea adevărată a parametrului <span class="math inline">\(\Theta\)</span>: <span class="math display">\[E \left\{ \hat{\Theta} \right\} = \Theta\]</span></p>
<p>Un estimator este <strong>deplasat</strong> atunci când valoarea medie a estimatorului diferă de valoarea adevărată a parametrului <span class="math inline">\(\Theta\)</span>. Diferența <span class="math inline">\(E \left\{ \hat{\Theta} \right\} - \Theta\)</span> reprezintă <strong>deplasarea</strong> estimatorului.</p>
<p>::: {.callout-tip icon=false title=“Exemplu”} ### Exemplu</p>
<p>Exemplu: pentru un semnal constant <span class="math inline">\(s_\Theta(t) = A\)</span>, afectat de zgomot Gaussian cu media 0, estimatorul de plauzibilitate maximă este <span class="math display">\[\hat{A}_{ML} = \frac{1}{N}\sum_i r_i\]</span>.</p>
<p>Pentru a vedea dacă acesta este deplasat sau nu, calculăm media estimatorului: <span class="math display">\[\begin{split}
E \left\{ \hat{A}_{ML} \right\} =&amp; \frac{1}{N}E \left\{ \sum_i r_i \right\} \\
=&amp; \frac{1}{N} \sum_{i=1}^N E \left\{ r_i \right\} \\
=&amp; \frac{1}{N} \sum_{i=1}^N E \left\{ A + zgomot \right\} \\
=&amp; \frac{1}{N} \sum_{i=1}^N A \\
=&amp; A
\end{split}\]</span></p>
<p>Așadar, acest estimator este nedeplasat, întrucât valoarea medie a estimatorului <span class="math inline">\(\hat{A}_{ML}\)</span> este egală chiar cu valoarea reală a parametrului <span class="math inline">\(A\)</span>.</p>
<p><strong>Varianța</strong> unui estimator măsoară “abaterile” estimatorului în jurul valorii medii (ca la orice variabolă aleatoare, de altfel).</p>
<p>Dacă un estimator are <strong>varianța mare</strong>, valoarea estimată poate fi departe de cea reală, chiar daca estimatorul este nedeplasat.</p>
<p>De obicei se preferă estimatori cu <strong>varianță mică</strong>, tolerându-se o eventuală mică deplasare.</p>
<p>O ilustrare a deplasării și a varianței estimatorilor este în figura de mai jos.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/BiasVariance.png" class="img-fluid figure-img" style="width:65.0%"></p>
<p></p><figcaption class="figure-caption">Deplasarea și varianța estimatorilor</figcaption><p></p>
</figure>
</div>
</section>
<section id="estimare-bayesiană" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="estimare-bayesiană"><span class="header-section-number">7.5</span> Estimare Bayesiană</h2>
<section id="distribuția-a-posteriori" class="level3" data-number="7.5.1">
<h3 data-number="7.5.1" class="anchored" data-anchor-id="distribuția-a-posteriori"><span class="header-section-number">7.5.1</span> Distribuția <em>a posteriori</em></h3>
<p>Estimarea Bayesiană reprezintă echivalentul din estimare pentru criteriile de decizie MPE și MR. În acest sens, se iau în calcul doi termeni suplimentari pe lângă <span class="math inline">\(w(\vec{r} | \Theta\)</span>:</p>
<ul>
<li>o distribuție <em>a priori</em> <span class="math inline">\(w(\Theta)\)</span></li>
<li>opțional, o funcție de cost</li>
</ul>
<p>Elementul cheie în estimarea Bayesiană îl reprezintă <strong>distribuția <em>a posteriori</em></strong> a lui <span class="math inline">\(\Theta\)</span>, date fiind observațiile <span class="math inline">\(\vec{r}\)</span>. Aceasta se scrie folosind regula lui Bayes, în felul următor:</p>
<p><span class="math display">\[w(\Theta | \vec{r}) = \frac{w(\vec{r} | \Theta) \cdot w(\Theta)}{w(\vec{r})}\]</span></p>
<p>Distribuția <em>a posteriori</em> <span class="math inline">\(w(\Theta | \vec{r})\)</span> este o densitate de probabilitate care descrie probabilitatea ca parametrul nostru necunoscut să aibă, în realitate, o valoare <span class="math inline">\(\Theta\)</span>.</p>
<p>Semnificația termenilor din această definiție este următoarea:</p>
<ul>
<li><span class="math inline">\(\Theta\)</span> este parametrul necunoscut;</li>
<li><span class="math inline">\(\vec{r}\)</span> este vectorul de observații care e luat în calcul în estimare;</li>
<li><span class="math inline">\(w(\Theta | \vec{r})\)</span> este distribuția <em>a posteriori</em>, adică densitatea de probabilitate ca parametrul să aibă valoarea <span class="math inline">\(\Theta\)</span></li>
<li><span class="math inline">\(w(\vec{r} | \Theta)\)</span> este funcția de plauzibilitate</li>
<li><span class="math inline">\(w(\Theta)\)</span> este distribuția <strong>a priori</strong> a lui <span class="math inline">\(\Theta\)</span>, care reflectă informația avută în prealabil cu privire la probabilitatea ca parametrul să ia o valoarea sau alta;</li>
<li><span class="math inline">\(w(\vec{r})\)</span> este distribuția <strong>a priori</strong> a lui <span class="math inline">\(\vec{r}\)</span>. Aceasta de consideră de obicei a fi constantă, și are rolul doar de a normaliza funcția astfel încât integrala totală din <span class="math inline">\(w(\Theta | \vec{r})\)</span> să fie egală 1, ca la orice densitate de probabilitate.</li>
</ul>
<p>Definiția de mai sus arată că, în general, estimarea lui <span class="math inline">\(\Theta\)</span> depinde atât de observațiile <span class="math inline">\(\vec{r}\)</span>, prin termenul <span class="math inline">\(w(\vec{r} | \Theta)\)</span>, cât și de informația “a priori” avută de la bun început despre <span class="math inline">\(\Theta\)</span>, prin termenul <span class="math inline">\(w(\Theta)\)</span>. Distribuția <em>a priori</em> reflectă, așadar, ceea ce știm de dinainte referitor la probabilitatea ca parametrul căutat să fie o valoare sau alte. Ea “trage” valoarea estimată înspre valori mai probabile.</p>
<p>Termenii <em>a priori</em> și <em>a posteriori</em> se raportează la momentul obținerii observațiilor <span class="math inline">\(\vec{r}\)</span></p>
<ul>
<li>distribuția <em>a priori</em> <span class="math inline">\(w(\Theta)\)</span> este probabilitatea ca o valoare oarecare <span class="math inline">\(\Theta\)</span> să fie cea corectă, <strong>înaintea</strong> obținerii observațiilor, deci fără a le lua în considerare;</li>
<li>distribuția <em>a posteriori</em> <span class="math inline">\(w(\Theta | \vec{r})\)</span> este probabilitatea ca o valoare oarecare <span class="math inline">\(\Theta\)</span> să fie cea corectă, <strong>după</strong> obținerea observațiilor, deci luându-le în calcul și pe acestea.</li>
</ul>
</section>
<section id="estimatorul-map" class="level3" data-number="7.5.2">
<h3 data-number="7.5.2" class="anchored" data-anchor-id="estimatorul-map"><span class="header-section-number">7.5.2</span> Estimatorul MAP</h3>
<p>Distribuția <em>a posteriori</em> <span class="math inline">\(w(\Theta | \vec{r})\)</span> ne dă probabilitatea fiecărei valori reale de a fi valoarea corectă a parametrului nostru. Așadar, dacă trebuie să alegem o singură valoare, care este exact valoarea estimată?</p>
<p>Estimatorul <strong>Maximum A Posteriori (MAP)</strong> definește valoarea estimată ca fiind valoarea <span class="math inline">\(\Theta\)</span> în care distribuția <em>a posteriori</em> <span class="math inline">\(w(\Theta | \vec{r})\)</span> este maximă: <span class="math display">\[\hat{\Theta}_{MAP} = \arg\max_\Theta w(\Theta | \vec{r}) = \arg\max_\Theta w(\vec{r} | \Theta) \cdot w(\Theta)\]</span></p>
<p>Ca interpretare, putem înțelege acest lucru ca fiind alegerea valorii care are probabilitate (plauzibilitate, mai corect spus) maximă, dar, spre deosebire de estimarea ML, ne raportăm la distribuția <em>a posteriori</em>, care ia în calcul pe lângă observații și distribuția <em>a priori</em>.</p>
<p>Practic, estimatorul MAP maximizează <strong>produsul</strong> dintre plauzibilitate <span class="math inline">\(w(\vec{r} | \Theta)\)</span> și <strong>distribuția <em>a priori</em> <span class="math inline">\(w(\Theta)\)</span></strong>.</p>
<p>Exemplu: Imagine</p>
<p>Relație dintre estimarea ML și estimarea MAP se observă din alăturarea celor două expresii:</p>
<ul>
<li><p>Estimatorul ML: <span class="math display">\[\arg\max w(\vec{r} | \Theta)\]</span></p></li>
<li><p>Estimatorul MAP: <span class="math display">\[\arg\max w(\vec{r} | \Theta) \cdot w(\Theta)\]</span></p></li>
<li><p>Estimatorul ML este un caz particular de MAP pentru cazul în care <span class="math inline">\(w(\Theta)\)</span> e constant, adicî atunci cand toate valorile lui <span class="math inline">\(\Theta\)</span> sunt <em>a priori</em> echiprobabile. Cu alte cuvinte, nu avem extra informații despre valoarea lui <span class="math inline">\(\Theta\)</span> în afara observațiilor propriu-zise.</p></li>
</ul>
<p>Relația cu detecția semnalelor se observă din alăturarea cu criteriul de decizie MPE.</p>
<ul>
<li><p><strong>Criteriul de decizie MPE</strong> alege ipoteza pentru care <span class="math inline">\(w(r | H_i)\cdot P(H_i)\)</span> este mai mare: <span class="math display">\[w(r | H_1)\cdot P(H_1) \grtlessH w(r | H_0) P(H_0)\]</span></p></li>
<li><p><strong>Estimarea MAP</strong>: se alege valoarea care maximizează <span class="math inline">\(w(\vec{r} | \Theta) \cdot w(\Theta)\)</span></p></li>
<li><p>Este același principiu, dar la decizie există doar două alternative, iar la estimare se alege o valoarea reală.</p></li>
</ul>
</section>
<section id="funcții-de-cost" class="level3" data-number="7.5.3">
<h3 data-number="7.5.3" class="anchored" data-anchor-id="funcții-de-cost"><span class="header-section-number">7.5.3</span> Funcții de cost</h3>
<p>Pentru a găsi un echivalent și pentru criteriul MR, vom introduce conceptul de funcție de cost, care generalizează costurile <span class="math inline">\(C_{ij}\)</span> de la decizii.</p>
<p><strong>Eroarea de estimare</strong> reprezintă diferența între estimatul <span class="math inline">\(\hat{\Theta}\)</span> și valoarea reală <span class="math inline">\(\Theta\)</span> <span class="math display">\[\epsilon = \hat{\Theta} - \Theta\]</span></p>
<p><strong>Funcția de cost <span class="math inline">\(C(\epsilon)\)</span></strong> este o funcție care atribuie un anume cost fiecărei erori de estimare posibilă:</p>
<ul>
<li>când <span class="math inline">\(\epsilon = 0\)</span>, costul <span class="math inline">\(C(0) = 0\)</span>, întrucât nu există deloc o eroare de estimare</li>
<li>pentru erori de estimare <span class="math inline">\(\epsilon\)</span> mici, valorile funcției de cost sunt mici</li>
<li>pentru erori de estimare <span class="math inline">\(\epsilon\)</span> mari, funcția de cost are în general valori mari</li>
</ul>
<p>În general, funcția de cost poate fi aleasă oricum. În practică, se folosesc o serie de funcții de cost uzuale:</p>
<ol type="1">
<li><p>Funcția de cost pătratică: <span class="math display">\[C(\epsilon) = \epsilon^2 = \left( \hat{\Theta} - \Theta \right)^2\]</span></p></li>
<li><p>Funcția de cost uniformă: <span class="math display">\[C(\epsilon) = \begin{cases}
0, \text{ if } |\epsilon| = |\hat{\Theta} - \Theta | \leq E \\
1, \text{ if } |\epsilon| = |\hat{\Theta} - \Theta | &gt; E \\
\end{cases}\]</span></p></li>
<li><p>Funcția de cost liniară: <span class="math display">\[C(\epsilon) = |\epsilon| = | \hat{\Theta} - \Theta |\]</span></p></li>
</ol>
<p>Funcția de cost <span class="math inline">\(C(\epsilon)\)</span> reprezintă echivalentul costurilor <span class="math inline">\(C_{ij}\)</span> de la detecție. Dacă la detecție aveam doar 4 valori posibile (<span class="math inline">\(C_{00}\)</span>, <span class="math inline">\(C_{01}\)</span>, <span class="math inline">\(C_{10}\)</span>, <span class="math inline">\(C_{11}\)</span>) întrucât existau doar 4 scenarii posibile, în cazul estimării avem un cost pentru fiecare eroare posibilă <span class="math inline">\(\epsilon\)</span>.</p>
<p>Importanța funcției de cost rezidă în faptul că ea dictează, de fapt, ce valoare anume alegem din distribuția <span class="math inline">\(w(\Theta | \vec{r})\)</span>.</p>
<div class="callout-tip callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Exemplu: funcției de cost asimetrică
</div>
</div>
<div class="callout-body-container callout-body" title="Exemplu">
<p>De exemplu, fie distribuția <em>a posteriori</em> următoare:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/UnbalancedPosterior.png" style="height:35.0%" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Asymmetrical posterior distribution</figcaption><p></p>
</figure>
</div>
<ul>
<li><p>Care este estimatorul MAP?</p></li>
<li><p>Dar dacă avem funcția de cost următoare:</p>
<ul>
<li>dacă estimarea <span class="math inline">\(\hat{\Theta}\)</span> este &lt; valoarea reală <span class="math inline">\(\Theta\)</span>, te costă 1000 $</li>
<li>dacă estimarea <span class="math inline">\(\hat{\Theta}\)</span> este &gt; valoarea reală <span class="math inline">\(\Theta\)</span>, platești 1 $</li>
<li>schimbăm valoarea estimată ? :)</li>
</ul></li>
</ul>
</div>
</div>
</section>
<section id="estimatorul-epmm" class="level3" data-number="7.5.4">
<h3 data-number="7.5.4" class="anchored" data-anchor-id="estimatorul-epmm"><span class="header-section-number">7.5.4</span> Estimatorul EPMM</h3>
<p>Distribuția <em>a posteriori</em> <span class="math inline">\(w(\Theta | \vec{r})\)</span> dă probabilitatea fiecărei valori <span class="math inline">\(\hat{\Theta}\)</span> de a fi cea corectă, dar alegerea unei anume valori estimate <span class="math inline">\(\hat{\Theta}\)</span> implică o anume eroare <span class="math inline">\(\epsilon\)</span> care are un anumit cost <span class="math inline">\(C(\epsilon)\)</span></p>
<p><strong>Riscul</strong> se definește ca valoarea medie a costului <span class="math inline">\(C(\epsilon\)</span>): <span class="math display">\[R = \int_{-\infty}^\infty C(\epsilon) w(\Theta | \vec{r}) d\Theta\]</span> Relația de mai sus este relația care definește orice valoare medie (suma dintra valorile individuale înmulțite cu probabilitatea lor). Aici mediem valorile costului <span class="math inline">\(C(\epsilon)\)</span>, iar probabilitatea fiecărui cost este de fapt probabilitatea valorilor <span class="math inline">\(\Theta\)</span> care conduc la un anume <span class="math inline">\(\epsilon\)</span> și astfel la fiecare valoare de cost, așadar <span class="math inline">\(w(\Theta | \vec{r})\)</span>.</p>
<p>În estimarea Bayesiană, valoarea estimată <span class="math inline">\(\hat{\Theta}\)</span> se alege ca fiind valoarea <span class="math inline">\(\hat{\Theta}\)</span> care <strong>minimizează costul mediu <span class="math inline">\(R\)</span></strong>: <span class="math display">\[\hat{\Theta} = \arg\min_\Theta \int_{-\infty}^\infty C(\epsilon) w(\Theta | \vec{r}) d\Theta\]</span></p>
<p>În această relație se înlocuiește <span class="math inline">\(C(\epsilon)\)</span> cu funcția de cost dorită. Pentru a găsi valoarea minimă se derivează după <span class="math inline">\(\hat{\Theta}\)</span>:</p>
<p>(Atenție: se derivează după <span class="math inline">\(\hat{\Theta}\)</span>, nu <span class="math inline">\(\Theta\)</span>, întrucât <span class="math inline">\(\hat{\Theta}\)</span> este valoarea pe care o căutam!.</p>
<p>Pentru funcția de cost pătratică, <span class="math inline">\(C(\epsilon) = \epsilon^2 = \left( \hat{\Theta} - \Theta \right)^2\)</span>, avem: <span class="math display">\[R = \int_{-\infty}^\infty (\hat{\Theta} - \Theta)^2 w(\Theta | \vec{r}) d\Theta\]</span> Se caută <span class="math inline">\(\hat{\Theta}\)</span> care minimizează <span class="math inline">\(R\)</span>, deci derivăm: <span class="math display">\[\frac{dR}{d\hat{\Theta}} = 2 \int_{-\infty}^\infty (\hat{\Theta} - \Theta) w(\Theta | \vec{r}) d\Theta = 0\]</span> Echivalent cu <span class="math display">\[\hat{\Theta} \underbrace{\int_{-\infty}^\infty w(\Theta | \vec{r})}_1 d\Theta = \int_{-\infty}^\infty \Theta w(\Theta | \vec{r}) d\Theta\]</span></p>
<p>Se ajunge în acest fel la relația care definește estimatorul de <strong>eroare pătratică medie minimă (EPMM) (“Minimum Mean Squared Error, MMSE”)</strong>: <span class="math display">\[\hat{\Theta}_{EPMM} = \int_{-\infty}^\infty \Theta \cdot w(\Theta | \vec{r}) d\Theta\]</span></p>
<p><strong>Estimatorul EPMM</strong>: estimatorul <span class="math inline">\(\hat{\Theta}\)</span> este <strong>valoarea medie</strong> a distribuției <em>a posteriori</em> <span class="math inline">\(w(\Theta | \vec{r})\)</span> <span class="math display">\[\hat{\Theta}_{EPMM} = \int_{-\infty}^\infty \Theta \cdot w(\Theta | \vec{r}) d\Theta\]</span></p>
<ul>
<li>EPMM = “Eroare Pătratică Medie Minimă”</li>
<li>valoarea medie = sumă (integrală) din fiecare <span class="math inline">\(\Theta\)</span> ori probabilitatea sa <span class="math inline">\(w(\Theta | \vec{r})\)</span></li>
</ul>
<p>Estimatprul EPMM se obține din distribuția <em>a posteriori</em> <span class="math inline">\(w(\Theta | \vec{r})\)</span>, considerând funcția de cost pătratică</p>
</section>
<section id="estimatorul-map-1" class="level3" data-number="7.5.5">
<h3 data-number="7.5.5" class="anchored" data-anchor-id="estimatorul-map-1"><span class="header-section-number">7.5.5</span> Estimatorul MAP</h3>
<ul>
<li><p>Dacă funcția de cost este uniformă <span class="math display">\[C(\epsilon) = \begin{cases}
  0, \text{ if } |\epsilon| = |\hat{\Theta} - \Theta | \leq E \\
  1, \text{ if } |\epsilon| = |\hat{\Theta} - \Theta | &gt; E \\
  \end{cases}\]</span></p></li>
<li><p>Știm că <span class="math inline">\(\Theta = \hat{\Theta} - \epsilon\)</span></p></li>
<li><p>Se obține <span class="math display">\[\begin{split}
  R =&amp; \int_{-\infty}^{\hat{\Theta}-E} w(\Theta | \vec{r}) d\Theta + \int_{\hat{\Theta} + E}^\infty w(\Theta | \vec{r}) d\Theta \\
  R =&amp; 1 - \int_{\hat{\Theta}-E}^{\hat{\Theta}+E} w(\Theta | \vec{r}) d\Theta
  \end{split}\]</span></p></li>
</ul>
</section>
<section id="estimatorul-map-2" class="level3" data-number="7.5.6">
<h3 data-number="7.5.6" class="anchored" data-anchor-id="estimatorul-map-2"><span class="header-section-number">7.5.6</span> Estimatorul MAP</h3>
<ul>
<li><p>Pentru minimizarea <span class="math inline">\(R\)</span>, trebuie să maximizăm <span class="math inline">\(\int_{\hat{\Theta}-E}^{\hat{\Theta}+E} w(\Theta | \vec{r}) d\Theta\)</span>, integrala din jurul punctului <span class="math inline">\(\hat{\Theta}\)</span></p></li>
<li><p>Pentru <span class="math inline">\(E\)</span> foarte mic, funcția <span class="math inline">\(w(\Theta | \vec{r})\)</span> este aproximativ constantă, deci se va alege punctul unde funcția este maximă</p></li>
<li><p><strong>Estimatorul Maximum A Posteriori (MAP)</strong> = valoarea <span class="math inline">\(\hat{\Theta}\)</span> care maximizează <span class="math inline">\(w(\Theta | \vec{r})\)</span> <span class="math display">\[\hat{\Theta}_{MAP} = \arg\max_\Theta w(\Theta | \vec{r}) = \arg\max\Theta w(\vec{r} | \Theta) \cdot w(\Theta)\]</span></p></li>
</ul>
</section>
<section id="interpretare" class="level3" data-number="7.5.7">
<h3 data-number="7.5.7" class="anchored" data-anchor-id="interpretare"><span class="header-section-number">7.5.7</span> Interpretare</h3>
<ul>
<li><p>Estimatorul MAP: <span class="math inline">\(\hat{\Theta}\)</span> = valoarea care maximizează distribuția <em>a posteriori</em></p></li>
<li><p>Estimatorul EPMM: <span class="math inline">\(\hat{\Theta}\)</span> = valoarea medie a distribuției <em>a posteriori</em></p></li>
</ul>
<div id="id" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/MAPvsMMSE.png" class="class img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Estimatorul MAP vs EPMM(MMSE)</figcaption><p></p>
</figure>
</div>
<p><em>[image from https://allenlu2007.wordpress.com]</em></p>
</section>
<section id="relația-între-estim.-map-and-epmm" class="level3" data-number="7.5.8">
<h3 data-number="7.5.8" class="anchored" data-anchor-id="relația-între-estim.-map-and-epmm"><span class="header-section-number">7.5.8</span> Relația între estim. MAP and EPMM</h3>
<ul>
<li><p>Estimatorul MAP = minimizează costul mediu, folosind funcția de cost uniformă</p>
<ul>
<li>ca le detecție: criteriul MPE = criteriul MR când costurile sunt la fel</li>
</ul></li>
<li><p>Estimatorul EPMM = minimizează costul mediu, folosind funcția de cost pătratică</p>
<ul>
<li>similar cu criteriul MR, dar la estimare</li>
</ul></li>
</ul>
</section>
<section id="exercițiu" class="level3" data-number="7.5.9">
<h3 data-number="7.5.9" class="anchored" data-anchor-id="exercițiu"><span class="header-section-number">7.5.9</span> Exercițiu</h3>
<p>Exercițiu: valoare constantă, 1 măsurătoare, zgomot Gaussian același <span class="math inline">\(\sigma\)</span></p>
<ul>
<li>Vrem să estimam temperatura de astăzi din Sahara</li>
<li>Termometrul indică 40 grade, dar valoarea este afectată de zgomot Gaussian <span class="math inline">\(\mathcal{N}(0, \sigma^2=2)\)</span> (termometru ieftin)</li>
<li>Se știe că de obicei în această perioadă a anului temperatura este în jur de 35 grade, cu o distribuție Gaussiană <span class="math inline">\(\mathcal{N}(35, \sigma^2 = 2)\)</span>.</li>
<li>Estimați valoarea reală a temperaturii folosind estimarea ML, MAP și EPMM(MMSE)</li>
</ul>
</section>
<section id="exercițiu-1" class="level3" data-number="7.5.10">
<h3 data-number="7.5.10" class="anchored" data-anchor-id="exercițiu-1"><span class="header-section-number">7.5.10</span> Exercițiu</h3>
<p>Exercițiu: valoare constantă, 1 măsurătoare, zgomot Gaussian același <span class="math inline">\(\sigma\)</span></p>
<ul>
<li>Dacă avem trei termometre, care indică 40, 38, 41 grade?</li>
</ul>
<p>Exercițiu: valoare constantă, 1 măsurătoare, zgomot Gaussian <span class="math inline">\(\sigma\)</span> diferit</p>
<ul>
<li>Dacă temperatura în această perioadă a anului are distribuție Gaussiană <span class="math inline">\(\mathcal{N}(35, \sigma_2^2 = 3)\)</span>
<ul>
<li>cu varianță diferită, <span class="math inline">\(\sigma_2 \neq \sigma\)</span></li>
</ul></li>
</ul>
</section>
<section id="semnal-oarecare-în-zgomot-gaussian-awgn" class="level3" data-number="7.5.11">
<h3 data-number="7.5.11" class="anchored" data-anchor-id="semnal-oarecare-în-zgomot-gaussian-awgn"><span class="header-section-number">7.5.11</span> Semnal oarecare în zgomot Gaussian (AWGN)</h3>
<ul>
<li><p>Fie semnalul original “curat” <span class="math inline">\(s_\Theta(t)\)</span></p></li>
<li><p>Zgomotul este Gaussian (AWGN) <span class="math inline">\(\mathcal{N}(\mu=0, \sigma^2)\)</span></p></li>
<li><p>Ca în cazul estimării de plauzibilitate maximă, funcția de plauzibilitate este: <span class="math display">\[\begin{split}
w(\vec{r} | \Theta) =&amp;  \frac{1}{\sigma \sqrt{2 \pi}} e^{- \frac{\sum(r_i - s_\Theta(t_i))^2}{2 \sigma^2}}
\end{split}\]</span></p></li>
<li><p>Dar acum aceasta <strong>se înmulțește cu <span class="math inline">\(w(\Theta)\)</span></strong> <span class="math display">\[w(\vec{r} | \Theta) \cdot w(\Theta)\]</span></p></li>
</ul>
</section>
<section id="semnal-oarecare-în-zgomot-gaussian-awgn-1" class="level3" data-number="7.5.12">
<h3 data-number="7.5.12" class="anchored" data-anchor-id="semnal-oarecare-în-zgomot-gaussian-awgn-1"><span class="header-section-number">7.5.12</span> Semnal oarecare în zgomot Gaussian (AWGN)</h3>
<ul>
<li><p>Estimatorul MAP estimator este cel care maximizează produsul <span class="math display">\[\hat{\Theta}_{MAP} = \arg\max w(\vec{r} | \Theta) w(\Theta)\]</span></p></li>
<li><p>Logaritmând: <span class="math display">\[\begin{split}
\hat{\Theta}_{MAP} =&amp; \arg\max \ln \left( w(\vec{r} | \Theta) \right) + \ln \left( w(\Theta) \right) \\
=&amp; \arg\max - \frac{\sum(r_i - s_\Theta(t_i))^2}{2 \sigma^2} + \ln \left(w(\Theta)\right)
\end{split}\]</span></p></li>
</ul>
</section>
<section id="distribuție-a-priori-gaussiană" class="level3" data-number="7.5.13">
<h3 data-number="7.5.13" class="anchored" data-anchor-id="distribuție-a-priori-gaussiană"><span class="header-section-number">7.5.13</span> Distribuție “a priori” Gaussiană</h3>
<ul>
<li><p>Dacă distribuția “a priori” este de asemenea Gaussiană <span class="math inline">\(\mathcal{N}(\mu_\Theta, \sigma_\Theta^2)\)</span> <span class="math display">\[ \ln \left(w(\Theta)\right) = - \frac{\sum(\Theta - \mu_\Theta)^2}{2 \sigma_\Theta^2}\]</span></p></li>
<li><p>Estimatorul MAP devine <span class="math display">\[ \hat{\Theta}_{MAP} = \arg\min \frac{\sum(r_i - s_\Theta(t_i))^2}{2 \sigma^2} + \frac{\sum(\Theta - \mu_\Theta)^2}{2 \sigma_\Theta^2}\]</span></p></li>
<li><p>Poate fi rescris <span class="math display">\[ \hat{\Theta}_{MAP} = \arg\min d(\vec{r},s_\Theta)^2 + \underbrace{\frac{\sigma^2}{\sigma_\Theta^2}}_\lambda \cdot d(\Theta, \mu_\Theta)^2\]</span></p></li>
</ul>
</section>
<section id="interpretare-1" class="level3" data-number="7.5.14">
<h3 data-number="7.5.14" class="anchored" data-anchor-id="interpretare-1"><span class="header-section-number">7.5.14</span> Interpretare</h3>
<ul>
<li><p>Estimatorul MAP în zgomot Gaussian și cu distribuție “a priori” Gaussiană <span class="math display">\[\hat{\Theta}_{MAP} = \arg\min d(\vec{r},s_\Theta)^2 + \underbrace{\frac{\sigma^2}{\sigma_\Theta^2}}_\lambda \cdot d(\Theta, \mu_\Theta)^2\]</span></p></li>
<li><p><span class="math inline">\(\hat{\Theta}_{MAP}\)</span> este apropiat de valoarea medie <span class="math inline">\(\mu_\Theta\)</span> și de asemenea face ca semnalul adevărat să fie apropiat de eșantioanele recepționate <span class="math inline">\(\vec{r}\)</span></p>
<ul>
<li>Exemplu: “caut locuință aproape de serviciu dar și aproape de Mall”</li>
<li><span class="math inline">\(\lambda\)</span> controlează importanța relativă a celor doi termeni</li>
</ul></li>
<li><p>Cazuri particulare</p>
<ul>
<li><span class="math inline">\(\sigma_\Theta\)</span> foarte mic = distribuția “a priori” este foarte specifică (îngustă) = <span class="math inline">\(\lambda\)</span> mare = termenul al doilea este dominant = <span class="math inline">\(\hat{\Theta}_{MAP}\)</span> foarte apropiat de <span class="math inline">\(\mu_\Theta\)</span></li>
<li><span class="math inline">\(\sigma_\Theta\)</span> foarte mare = distribuția “a priori” este foarte nespecifică = <span class="math inline">\(\lambda\)</span> mic = primul termen este dominant = <span class="math inline">\(\hat{\Theta}_{MAP}\)</span> apropiat de estimatorul de plauzibilitate maximă</li>
</ul></li>
</ul>
</section>
<section id="aplicații" class="level3" data-number="7.5.15">
<h3 data-number="7.5.15" class="anchored" data-anchor-id="aplicații"><span class="header-section-number">7.5.15</span> Aplicații</h3>
<ul>
<li><p>În general, aplicațiile practice:</p>
<ul>
<li>utilizează diverse tipuri de distribuții “a priori”</li>
<li>estimează <strong>mai mulți parametri</strong> (un vector de parametri)</li>
</ul></li>
<li><p>Aplicații</p>
<ul>
<li>reducerea zgomotului din semnale</li>
<li>restaurarea semnalelor (parți lipsă din imagini, imagini <em>blurate</em> etc)</li>
<li>compresia semnalelor</li>
</ul></li>
</ul>
</section>
<section id="aplicații-practice" class="level3" data-number="7.5.16">
<h3 data-number="7.5.16" class="anchored" data-anchor-id="aplicații-practice"><span class="header-section-number">7.5.16</span> Aplicații practice</h3>
<ol type="1">
<li>Urmărirea unui obiect (“single object tracking”) prin filtrare Kalman</li>
</ol>
<ul>
<li><p>urmărirea unui obiect prin măsurători succesive (e.g.&nbsp;din imagini succesive)</p></li>
<li><p>la fiecare nouă măsurătoare avem două distribuții ale poziției:</p>
<ul>
<li>cea dată de măsurătoare respectivă, <span class="math inline">\(w(r | \Theta)\)</span></li>
<li>cea prezisă pe baza poziției și vitezei de data trecută</li>
<li>ambele presupuse a fi Gaussiene, caracterizate doar prin medie și varianță</li>
</ul></li>
<li><p>cele două se combină prin regula lui Bayes =&gt; o distribuție mai precisă <span class="math inline">\(w(\Theta | r)\)</span>, tot Gaussiană</p></li>
<li><p>poziția exactă se estimează prin EPMM (media lui <span class="math inline">\(w(\Theta | r)\)</span></p></li>
<li><p><span class="math inline">\(w(\Theta | r)\)</span> prezice poziția de la momentul următor</p></li>
</ul>
</section>
<section id="single-object-tracking" class="level3" data-number="7.5.17">
<h3 data-number="7.5.17" class="anchored" data-anchor-id="single-object-tracking"><span class="header-section-number">7.5.17</span> Single object tracking</h3>
</section>
<section id="single-object-tracking-1" class="level3" data-number="7.5.18">
<h3 data-number="7.5.18" class="anchored" data-anchor-id="single-object-tracking-1"><span class="header-section-number">7.5.18</span> Single object tracking</h3>
</section>
<section id="aplicații-practice-1" class="level3" data-number="7.5.19">
<h3 data-number="7.5.19" class="anchored" data-anchor-id="aplicații-practice-1"><span class="header-section-number">7.5.19</span> Aplicații practice</h3>
<ol start="2" type="1">
<li>Constrained Least Squares (CLS) image restoration</li>
</ol>
<ul>
<li><p>Avem o imagine <span class="math inline">\(I\)</span> afectată de erori (zgomot, pixeli lipsă, blurare) <span class="math display">\[I_{zg} = I_{true} + Z\]</span></p></li>
<li><p>Estimăm imaginea originală prin: <span class="math display">\[\hat{I_{true}} = argmin_{I} \|I - I_{zg}\|_2 + \lambda \cdot \|HighPass\lbrace I \rbrace\|_2\]</span></p></li>
<li><p>Exemple:</p>
<ul>
<li><p><a href="https://www.mathworks.com/help/images/deblurring-images-using-a-regularized-filter.html">https://www.mathworks.com/help/images/deblurring-images-using-a-regularized-filter.html</a></p></li>
<li><p><a href="https://demonstrations.wolfram.com/ImageRestorationForDegradedImages">https://demonstrations.wolfram.com/ImageRestorationForDegradedImages</a></p></li>
<li><p>Google it</p></li>
</ul></li>
</ul>
</section>
<section id="constrained-least-squares-cls-image-restoration" class="level3" data-number="7.5.20">
<h3 data-number="7.5.20" class="anchored" data-anchor-id="constrained-least-squares-cls-image-restoration"><span class="header-section-number">7.5.20</span> Constrained Least Squares (CLS) image restoration</h3>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./02_03_AlgoritmiPractici.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Algoritmi practici</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ex_Seminar01.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Seminar 01: Variabile aleatoare și probabilități</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>